{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, tensorflow as tf, pandas as pd, matplotlib.pyplot as plt\n",
    "import os\n",
    "import AnOxPePred_funcs as AOf\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data'\n",
    "result_path = 'Result'\n",
    "embedding_file = os.path.join(data_path, 'One-hot_encoding.txt')\n",
    "AO_p60 = pd.read_csv(os.path.join(data_path, '03_p60_AO_db.csv'), index_col=0)\n",
    "AO_p70 = pd.read_csv(os.path.join(data_path, '03_p70_AO_db.csv'), index_col=0)\n",
    "AO_p80 = pd.read_csv(os.path.join(data_path, '03_p80_AO_db.csv'), index_col=0)\n",
    "AO_p90 = pd.read_csv(os.path.join(data_path, '03_p90_AO_db.csv'), index_col=0)\n",
    "\n",
    "a_data = AO_p70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model (for playing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 842 samples, validate on 281 samples\n",
      "Epoch 1/800\n",
      "842/842 [==============================] - 2s 2ms/sample - loss: 0.2082 - acc: 0.5000 - val_loss: 0.1844 - val_acc: 0.5231\n",
      "Epoch 2/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.2009 - acc: 0.5772 - val_loss: 0.1755 - val_acc: 0.6940\n",
      "Epoch 3/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.1917 - acc: 0.7067 - val_loss: 0.1685 - val_acc: 0.8043\n",
      "Epoch 4/800\n",
      "842/842 [==============================] - 0s 368us/sample - loss: 0.1839 - acc: 0.8088 - val_loss: 0.1678 - val_acc: 0.9039\n",
      "Epoch 5/800\n",
      "842/842 [==============================] - 0s 378us/sample - loss: 0.1763 - acc: 0.8575 - val_loss: 0.1568 - val_acc: 0.9395\n",
      "Epoch 6/800\n",
      "842/842 [==============================] - 0s 388us/sample - loss: 0.1687 - acc: 0.9074 - val_loss: 0.1504 - val_acc: 0.9537\n",
      "Epoch 7/800\n",
      "842/842 [==============================] - 0s 387us/sample - loss: 0.1627 - acc: 0.9371 - val_loss: 0.1445 - val_acc: 0.9858\n",
      "Epoch 8/800\n",
      "842/842 [==============================] - 0s 378us/sample - loss: 0.1572 - acc: 0.9572 - val_loss: 0.1392 - val_acc: 0.9858\n",
      "Epoch 9/800\n",
      "842/842 [==============================] - 0s 376us/sample - loss: 0.1502 - acc: 0.9656 - val_loss: 0.1363 - val_acc: 0.9964\n",
      "Epoch 10/800\n",
      "842/842 [==============================] - 0s 416us/sample - loss: 0.1468 - acc: 0.9739 - val_loss: 0.1295 - val_acc: 0.9964\n",
      "Epoch 11/800\n",
      "842/842 [==============================] - 0s 398us/sample - loss: 0.1415 - acc: 0.9810 - val_loss: 0.1251 - val_acc: 0.9964\n",
      "Epoch 12/800\n",
      "842/842 [==============================] - 0s 392us/sample - loss: 0.1335 - acc: 0.9822 - val_loss: 0.1214 - val_acc: 0.9964\n",
      "Epoch 13/800\n",
      "842/842 [==============================] - 0s 393us/sample - loss: 0.1304 - acc: 0.9869 - val_loss: 0.1177 - val_acc: 0.9964\n",
      "Epoch 14/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.1256 - acc: 0.9881 - val_loss: 0.1140 - val_acc: 0.9964\n",
      "Epoch 15/800\n",
      "842/842 [==============================] - 0s 379us/sample - loss: 0.1226 - acc: 0.9893 - val_loss: 0.1097 - val_acc: 0.9964\n",
      "Epoch 16/800\n",
      "842/842 [==============================] - 0s 400us/sample - loss: 0.1181 - acc: 0.9893 - val_loss: 0.1091 - val_acc: 0.9964\n",
      "Epoch 17/800\n",
      "842/842 [==============================] - 0s 368us/sample - loss: 0.1150 - acc: 0.9893 - val_loss: 0.1040 - val_acc: 0.9964\n",
      "Epoch 18/800\n",
      "842/842 [==============================] - 0s 432us/sample - loss: 0.1113 - acc: 0.9893 - val_loss: 0.1016 - val_acc: 0.9964\n",
      "Epoch 19/800\n",
      "842/842 [==============================] - 0s 375us/sample - loss: 0.1098 - acc: 0.9893 - val_loss: 0.1002 - val_acc: 0.9964\n",
      "Epoch 20/800\n",
      "842/842 [==============================] - 0s 508us/sample - loss: 0.1070 - acc: 0.9893 - val_loss: 0.0982 - val_acc: 0.9964\n",
      "Epoch 21/800\n",
      "842/842 [==============================] - 0s 364us/sample - loss: 0.1044 - acc: 0.9893 - val_loss: 0.0963 - val_acc: 0.9964\n",
      "Epoch 22/800\n",
      "842/842 [==============================] - 0s 362us/sample - loss: 0.1038 - acc: 0.9893 - val_loss: 0.0956 - val_acc: 0.9964\n",
      "Epoch 23/800\n",
      "842/842 [==============================] - 0s 407us/sample - loss: 0.1016 - acc: 0.9893 - val_loss: 0.0930 - val_acc: 0.9964\n",
      "Epoch 24/800\n",
      "842/842 [==============================] - 0s 411us/sample - loss: 0.0994 - acc: 0.9893 - val_loss: 0.0924 - val_acc: 0.9964\n",
      "Epoch 25/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0990 - acc: 0.9893 - val_loss: 0.0935 - val_acc: 0.9964\n",
      "Epoch 26/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0972 - acc: 0.9893 - val_loss: 0.0904 - val_acc: 0.9964\n",
      "Epoch 27/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0962 - acc: 0.9893 - val_loss: 0.0894 - val_acc: 0.9964\n",
      "Epoch 28/800\n",
      "842/842 [==============================] - 0s 461us/sample - loss: 0.0950 - acc: 0.9893 - val_loss: 0.0888 - val_acc: 0.9964\n",
      "Epoch 29/800\n",
      "842/842 [==============================] - 0s 396us/sample - loss: 0.0947 - acc: 0.9893 - val_loss: 0.0894 - val_acc: 0.9964\n",
      "Epoch 30/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.0935 - acc: 0.9893 - val_loss: 0.0871 - val_acc: 0.9964\n",
      "Epoch 31/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0929 - acc: 0.9893 - val_loss: 0.0861 - val_acc: 0.9964\n",
      "Epoch 32/800\n",
      "842/842 [==============================] - 0s 399us/sample - loss: 0.0918 - acc: 0.9893 - val_loss: 0.0856 - val_acc: 0.9964\n",
      "Epoch 33/800\n",
      "842/842 [==============================] - 0s 370us/sample - loss: 0.0918 - acc: 0.9893 - val_loss: 0.0850 - val_acc: 0.9964\n",
      "Epoch 34/800\n",
      "842/842 [==============================] - 0s 361us/sample - loss: 0.0910 - acc: 0.9893 - val_loss: 0.0855 - val_acc: 0.9964\n",
      "Epoch 35/800\n",
      "842/842 [==============================] - 0s 362us/sample - loss: 0.0905 - acc: 0.9893 - val_loss: 0.0843 - val_acc: 0.9964\n",
      "Epoch 36/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0902 - acc: 0.9893 - val_loss: 0.0841 - val_acc: 0.9964\n",
      "Epoch 37/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0901 - acc: 0.9893 - val_loss: 0.0836 - val_acc: 0.9964\n",
      "Epoch 38/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0897 - acc: 0.9893 - val_loss: 0.0833 - val_acc: 0.9964\n",
      "Epoch 39/800\n",
      "842/842 [==============================] - 0s 343us/sample - loss: 0.0897 - acc: 0.9893 - val_loss: 0.0838 - val_acc: 0.9964\n",
      "Epoch 40/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0881 - acc: 0.9893 - val_loss: 0.0827 - val_acc: 0.9964\n",
      "Epoch 41/800\n",
      "842/842 [==============================] - 0s 363us/sample - loss: 0.0877 - acc: 0.9893 - val_loss: 0.0825 - val_acc: 0.9964\n",
      "Epoch 42/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0878 - acc: 0.9893 - val_loss: 0.0822 - val_acc: 0.9964\n",
      "Epoch 43/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0875 - acc: 0.9893 - val_loss: 0.0827 - val_acc: 0.9964\n",
      "Epoch 44/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0877 - acc: 0.9893 - val_loss: 0.0824 - val_acc: 0.9964\n",
      "Epoch 45/800\n",
      "842/842 [==============================] - 0s 400us/sample - loss: 0.0870 - acc: 0.9893 - val_loss: 0.0816 - val_acc: 0.9964\n",
      "Epoch 46/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0868 - acc: 0.9893 - val_loss: 0.0819 - val_acc: 0.9964\n",
      "Epoch 47/800\n",
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0865 - acc: 0.9893 - val_loss: 0.0815 - val_acc: 0.9964\n",
      "Epoch 48/800\n",
      "842/842 [==============================] - 0s 393us/sample - loss: 0.0863 - acc: 0.9893 - val_loss: 0.0813 - val_acc: 0.9964\n",
      "Epoch 49/800\n",
      "842/842 [==============================] - 0s 373us/sample - loss: 0.0860 - acc: 0.9893 - val_loss: 0.0809 - val_acc: 0.9964\n",
      "Epoch 50/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0854 - acc: 0.9893 - val_loss: 0.0806 - val_acc: 0.9964\n",
      "Epoch 51/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0865 - acc: 0.9893 - val_loss: 0.0811 - val_acc: 0.9964\n",
      "Epoch 52/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0854 - acc: 0.9893 - val_loss: 0.0807 - val_acc: 0.9964\n",
      "Epoch 53/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0853 - acc: 0.9893 - val_loss: 0.0804 - val_acc: 0.9964\n",
      "Epoch 54/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0852 - acc: 0.9893 - val_loss: 0.0804 - val_acc: 0.9964\n",
      "Epoch 55/800\n",
      "842/842 [==============================] - 0s 364us/sample - loss: 0.0852 - acc: 0.9893 - val_loss: 0.0802 - val_acc: 0.9964\n",
      "Epoch 56/800\n",
      "842/842 [==============================] - 0s 412us/sample - loss: 0.0848 - acc: 0.9893 - val_loss: 0.0798 - val_acc: 0.9964\n",
      "Epoch 57/800\n",
      "842/842 [==============================] - 0s 362us/sample - loss: 0.0843 - acc: 0.9893 - val_loss: 0.0799 - val_acc: 0.9964\n",
      "Epoch 58/800\n",
      "842/842 [==============================] - 0s 404us/sample - loss: 0.0848 - acc: 0.9893 - val_loss: 0.0800 - val_acc: 0.9964\n",
      "Epoch 59/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0842 - acc: 0.9893 - val_loss: 0.0797 - val_acc: 0.9964\n",
      "Epoch 60/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0841 - acc: 0.9893 - val_loss: 0.0796 - val_acc: 0.9964\n",
      "Epoch 61/800\n",
      "842/842 [==============================] - 0s 418us/sample - loss: 0.0841 - acc: 0.9893 - val_loss: 0.0801 - val_acc: 0.9964\n",
      "Epoch 62/800\n",
      "842/842 [==============================] - 0s 432us/sample - loss: 0.0835 - acc: 0.9893 - val_loss: 0.0795 - val_acc: 0.9964\n",
      "Epoch 63/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0841 - acc: 0.9893 - val_loss: 0.0794 - val_acc: 0.9964\n",
      "Epoch 64/800\n",
      "842/842 [==============================] - 0s 375us/sample - loss: 0.0831 - acc: 0.9893 - val_loss: 0.0795 - val_acc: 0.9964\n",
      "Epoch 65/800\n",
      "842/842 [==============================] - 0s 391us/sample - loss: 0.0833 - acc: 0.9893 - val_loss: 0.0791 - val_acc: 0.9964\n",
      "Epoch 66/800\n",
      "842/842 [==============================] - 0s 394us/sample - loss: 0.0829 - acc: 0.9893 - val_loss: 0.0789 - val_acc: 0.9964\n",
      "Epoch 67/800\n",
      "842/842 [==============================] - 0s 370us/sample - loss: 0.0831 - acc: 0.9893 - val_loss: 0.0799 - val_acc: 0.9964\n",
      "Epoch 68/800\n",
      "842/842 [==============================] - 0s 373us/sample - loss: 0.0829 - acc: 0.9893 - val_loss: 0.0793 - val_acc: 0.9964\n",
      "Epoch 69/800\n",
      "842/842 [==============================] - 0s 391us/sample - loss: 0.0824 - acc: 0.9893 - val_loss: 0.0796 - val_acc: 0.9964\n",
      "Epoch 70/800\n",
      "842/842 [==============================] - 0s 439us/sample - loss: 0.0836 - acc: 0.9893 - val_loss: 0.0798 - val_acc: 0.9964\n",
      "Epoch 71/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0832 - acc: 0.9893 - val_loss: 0.0789 - val_acc: 0.9964\n",
      "Epoch 72/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0822 - acc: 0.9893 - val_loss: 0.0791 - val_acc: 0.9964\n",
      "Epoch 73/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0828 - acc: 0.9893 - val_loss: 0.0788 - val_acc: 0.9964\n",
      "Epoch 74/800\n",
      "842/842 [==============================] - 0s 384us/sample - loss: 0.0823 - acc: 0.9893 - val_loss: 0.0792 - val_acc: 0.9964\n",
      "Epoch 75/800\n",
      "842/842 [==============================] - 0s 386us/sample - loss: 0.0819 - acc: 0.9893 - val_loss: 0.0787 - val_acc: 0.9964\n",
      "Epoch 76/800\n",
      "842/842 [==============================] - 0s 373us/sample - loss: 0.0823 - acc: 0.9893 - val_loss: 0.0785 - val_acc: 0.9964\n",
      "Epoch 77/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.0827 - acc: 0.9893 - val_loss: 0.0783 - val_acc: 0.9964\n",
      "Epoch 78/800\n",
      "842/842 [==============================] - 0s 363us/sample - loss: 0.0824 - acc: 0.9893 - val_loss: 0.0782 - val_acc: 0.9964\n",
      "Epoch 79/800\n",
      "842/842 [==============================] - 0s 400us/sample - loss: 0.0823 - acc: 0.9893 - val_loss: 0.0790 - val_acc: 0.9964\n",
      "Epoch 80/800\n",
      "842/842 [==============================] - 0s 391us/sample - loss: 0.0816 - acc: 0.9893 - val_loss: 0.0779 - val_acc: 0.9964\n",
      "Epoch 81/800\n",
      "842/842 [==============================] - 0s 392us/sample - loss: 0.0815 - acc: 0.9893 - val_loss: 0.0784 - val_acc: 0.9964\n",
      "Epoch 82/800\n",
      "842/842 [==============================] - 0s 407us/sample - loss: 0.0822 - acc: 0.9893 - val_loss: 0.0781 - val_acc: 0.9964\n",
      "Epoch 83/800\n",
      "842/842 [==============================] - 0s 379us/sample - loss: 0.0811 - acc: 0.9893 - val_loss: 0.0778 - val_acc: 0.9964\n",
      "Epoch 84/800\n",
      "842/842 [==============================] - 0s 351us/sample - loss: 0.0806 - acc: 0.9893 - val_loss: 0.0783 - val_acc: 0.9964\n",
      "Epoch 85/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0811 - acc: 0.9893 - val_loss: 0.0784 - val_acc: 0.9964\n",
      "Epoch 86/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0815 - acc: 0.9893 - val_loss: 0.0776 - val_acc: 0.9964\n",
      "Epoch 87/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0810 - acc: 0.9893 - val_loss: 0.0780 - val_acc: 0.9964\n",
      "Epoch 88/800\n",
      "842/842 [==============================] - 0s 388us/sample - loss: 0.0806 - acc: 0.9893 - val_loss: 0.0784 - val_acc: 0.9964\n",
      "Epoch 89/800\n",
      "842/842 [==============================] - 0s 382us/sample - loss: 0.0817 - acc: 0.9893 - val_loss: 0.0778 - val_acc: 0.9964\n",
      "Epoch 90/800\n",
      "842/842 [==============================] - 0s 388us/sample - loss: 0.0808 - acc: 0.9893 - val_loss: 0.0783 - val_acc: 0.9964\n",
      "Epoch 91/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0807 - acc: 0.9893 - val_loss: 0.0776 - val_acc: 0.9964\n",
      "Epoch 92/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0809 - acc: 0.9893 - val_loss: 0.0777 - val_acc: 0.9964\n",
      "Epoch 93/800\n",
      "842/842 [==============================] - 0s 382us/sample - loss: 0.0806 - acc: 0.9893 - val_loss: 0.0775 - val_acc: 0.9964\n",
      "Epoch 94/800\n",
      "842/842 [==============================] - 0s 466us/sample - loss: 0.0814 - acc: 0.9893 - val_loss: 0.0773 - val_acc: 0.9964\n",
      "Epoch 95/800\n",
      "842/842 [==============================] - 0s 413us/sample - loss: 0.0801 - acc: 0.9893 - val_loss: 0.0773 - val_acc: 0.9964\n",
      "Epoch 96/800\n",
      "842/842 [==============================] - 0s 403us/sample - loss: 0.0804 - acc: 0.9893 - val_loss: 0.0773 - val_acc: 0.9964\n",
      "Epoch 97/800\n",
      "842/842 [==============================] - 0s 499us/sample - loss: 0.0798 - acc: 0.9905 - val_loss: 0.0773 - val_acc: 0.9964\n",
      "Epoch 98/800\n",
      "842/842 [==============================] - 0s 377us/sample - loss: 0.0801 - acc: 0.9893 - val_loss: 0.0772 - val_acc: 0.9964\n",
      "Epoch 99/800\n",
      "842/842 [==============================] - 0s 430us/sample - loss: 0.0798 - acc: 0.9893 - val_loss: 0.0773 - val_acc: 0.9964\n",
      "Epoch 100/800\n",
      "842/842 [==============================] - 0s 431us/sample - loss: 0.0807 - acc: 0.9893 - val_loss: 0.0773 - val_acc: 0.9964\n",
      "Epoch 101/800\n",
      "842/842 [==============================] - 0s 591us/sample - loss: 0.0796 - acc: 0.9893 - val_loss: 0.0774 - val_acc: 0.9964\n",
      "Epoch 102/800\n",
      "842/842 [==============================] - 0s 386us/sample - loss: 0.0795 - acc: 0.9893 - val_loss: 0.0770 - val_acc: 0.9964\n",
      "Epoch 103/800\n",
      "842/842 [==============================] - 0s 388us/sample - loss: 0.0802 - acc: 0.9905 - val_loss: 0.0779 - val_acc: 0.9964\n",
      "Epoch 104/800\n",
      "842/842 [==============================] - 0s 392us/sample - loss: 0.0800 - acc: 0.9893 - val_loss: 0.0771 - val_acc: 0.9964\n",
      "Epoch 105/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0797 - acc: 0.9893 - val_loss: 0.0777 - val_acc: 0.9964\n",
      "Epoch 106/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0803 - acc: 0.9893 - val_loss: 0.0786 - val_acc: 0.9964\n",
      "Epoch 107/800\n",
      "842/842 [==============================] - 0s 376us/sample - loss: 0.0794 - acc: 0.9893 - val_loss: 0.0769 - val_acc: 0.9964\n",
      "Epoch 108/800\n",
      "842/842 [==============================] - 0s 372us/sample - loss: 0.0799 - acc: 0.9893 - val_loss: 0.0767 - val_acc: 0.9964\n",
      "Epoch 109/800\n",
      "842/842 [==============================] - 0s 362us/sample - loss: 0.0789 - acc: 0.9893 - val_loss: 0.0770 - val_acc: 0.9964\n",
      "Epoch 110/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.0796 - acc: 0.9893 - val_loss: 0.0771 - val_acc: 0.9964\n",
      "Epoch 111/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0790 - acc: 0.9905 - val_loss: 0.0771 - val_acc: 0.9964\n",
      "Epoch 112/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0793 - acc: 0.9893 - val_loss: 0.0768 - val_acc: 0.9964\n",
      "Epoch 113/800\n",
      "842/842 [==============================] - 0s 390us/sample - loss: 0.0797 - acc: 0.9893 - val_loss: 0.0770 - val_acc: 0.9964\n",
      "Epoch 114/800\n",
      "842/842 [==============================] - 0s 415us/sample - loss: 0.0798 - acc: 0.9893 - val_loss: 0.0770 - val_acc: 0.9964\n",
      "Epoch 115/800\n",
      "842/842 [==============================] - 0s 384us/sample - loss: 0.0786 - acc: 0.9893 - val_loss: 0.0765 - val_acc: 0.9964\n",
      "Epoch 116/800\n",
      "842/842 [==============================] - 0s 361us/sample - loss: 0.0795 - acc: 0.9893 - val_loss: 0.0777 - val_acc: 0.9964\n",
      "Epoch 117/800\n",
      "842/842 [==============================] - 0s 393us/sample - loss: 0.0789 - acc: 0.9893 - val_loss: 0.0765 - val_acc: 0.9964\n",
      "Epoch 118/800\n",
      "842/842 [==============================] - 0s 383us/sample - loss: 0.0797 - acc: 0.9893 - val_loss: 0.0774 - val_acc: 0.9964\n",
      "Epoch 119/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0786 - acc: 0.9893 - val_loss: 0.0766 - val_acc: 0.9964\n",
      "Epoch 120/800\n",
      "842/842 [==============================] - 0s 390us/sample - loss: 0.0781 - acc: 0.9905 - val_loss: 0.0769 - val_acc: 0.9964\n",
      "Epoch 121/800\n",
      "842/842 [==============================] - 0s 376us/sample - loss: 0.0793 - acc: 0.9893 - val_loss: 0.0775 - val_acc: 0.9964\n",
      "Epoch 122/800\n",
      "842/842 [==============================] - 0s 362us/sample - loss: 0.0779 - acc: 0.9893 - val_loss: 0.0768 - val_acc: 0.9964\n",
      "Epoch 123/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0783 - acc: 0.9893 - val_loss: 0.0768 - val_acc: 0.9964\n",
      "Epoch 124/800\n",
      "842/842 [==============================] - 0s 413us/sample - loss: 0.0787 - acc: 0.9893 - val_loss: 0.0763 - val_acc: 0.9964\n",
      "Epoch 125/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0782 - acc: 0.9893 - val_loss: 0.0767 - val_acc: 0.9964\n",
      "Epoch 126/800\n",
      "842/842 [==============================] - 1s 618us/sample - loss: 0.0785 - acc: 0.9893 - val_loss: 0.0763 - val_acc: 0.9964\n",
      "Epoch 127/800\n",
      "842/842 [==============================] - 0s 430us/sample - loss: 0.0782 - acc: 0.9905 - val_loss: 0.0769 - val_acc: 0.9964\n",
      "Epoch 128/800\n",
      "842/842 [==============================] - 0s 393us/sample - loss: 0.0783 - acc: 0.9893 - val_loss: 0.0765 - val_acc: 0.9964\n",
      "Epoch 129/800\n",
      "842/842 [==============================] - 0s 409us/sample - loss: 0.0781 - acc: 0.9905 - val_loss: 0.0775 - val_acc: 0.9964\n",
      "Epoch 130/800\n",
      "842/842 [==============================] - 0s 401us/sample - loss: 0.0785 - acc: 0.9893 - val_loss: 0.0765 - val_acc: 0.9964\n",
      "Epoch 131/800\n",
      "842/842 [==============================] - 0s 398us/sample - loss: 0.0781 - acc: 0.9893 - val_loss: 0.0761 - val_acc: 0.9964\n",
      "Epoch 132/800\n",
      "842/842 [==============================] - 0s 379us/sample - loss: 0.0781 - acc: 0.9905 - val_loss: 0.0763 - val_acc: 0.9964\n",
      "Epoch 133/800\n",
      "842/842 [==============================] - 0s 364us/sample - loss: 0.0780 - acc: 0.9893 - val_loss: 0.0762 - val_acc: 0.9964\n",
      "Epoch 134/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0776 - acc: 0.9905 - val_loss: 0.0767 - val_acc: 0.9964\n",
      "Epoch 135/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0779 - acc: 0.9893 - val_loss: 0.0768 - val_acc: 0.9964\n",
      "Epoch 136/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0780 - acc: 0.9905 - val_loss: 0.0764 - val_acc: 0.9964\n",
      "Epoch 137/800\n",
      "842/842 [==============================] - 0s 362us/sample - loss: 0.0773 - acc: 0.9905 - val_loss: 0.0763 - val_acc: 0.9964\n",
      "Epoch 138/800\n",
      "842/842 [==============================] - 0s 372us/sample - loss: 0.0778 - acc: 0.9905 - val_loss: 0.0757 - val_acc: 0.9964\n",
      "Epoch 139/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0781 - acc: 0.9893 - val_loss: 0.0768 - val_acc: 0.9964\n",
      "Epoch 140/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0778 - acc: 0.9905 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 141/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0783 - acc: 0.9893 - val_loss: 0.0762 - val_acc: 0.9964\n",
      "Epoch 142/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0768 - acc: 0.9905 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 143/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0770 - acc: 0.9893 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 144/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0776 - acc: 0.9905 - val_loss: 0.0758 - val_acc: 0.9964\n",
      "Epoch 145/800\n",
      "842/842 [==============================] - 0s 384us/sample - loss: 0.0787 - acc: 0.9905 - val_loss: 0.0765 - val_acc: 0.9964\n",
      "Epoch 146/800\n",
      "842/842 [==============================] - 0s 471us/sample - loss: 0.0778 - acc: 0.9893 - val_loss: 0.0757 - val_acc: 0.9964\n",
      "Epoch 147/800\n",
      "842/842 [==============================] - 0s 454us/sample - loss: 0.0770 - acc: 0.9905 - val_loss: 0.0762 - val_acc: 0.9964\n",
      "Epoch 148/800\n",
      "842/842 [==============================] - 0s 469us/sample - loss: 0.0768 - acc: 0.9881 - val_loss: 0.0771 - val_acc: 0.9964\n",
      "Epoch 149/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0785 - acc: 0.9905 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 150/800\n",
      "842/842 [==============================] - 0s 375us/sample - loss: 0.0765 - acc: 0.9905 - val_loss: 0.0762 - val_acc: 0.9964\n",
      "Epoch 151/800\n",
      "842/842 [==============================] - 0s 377us/sample - loss: 0.0770 - acc: 0.9905 - val_loss: 0.0762 - val_acc: 0.9964\n",
      "Epoch 152/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0766 - acc: 0.9905 - val_loss: 0.0758 - val_acc: 0.9964\n",
      "Epoch 153/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0769 - acc: 0.9905 - val_loss: 0.0758 - val_acc: 0.9964\n",
      "Epoch 154/800\n",
      "842/842 [==============================] - 0s 367us/sample - loss: 0.0775 - acc: 0.9905 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 155/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0767 - acc: 0.9905 - val_loss: 0.0764 - val_acc: 0.9964\n",
      "Epoch 156/800\n",
      "842/842 [==============================] - 0s 445us/sample - loss: 0.0767 - acc: 0.9893 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 157/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.0768 - acc: 0.9893 - val_loss: 0.0758 - val_acc: 0.9964\n",
      "Epoch 158/800\n",
      "842/842 [==============================] - 0s 344us/sample - loss: 0.0764 - acc: 0.9905 - val_loss: 0.0756 - val_acc: 0.9964\n",
      "Epoch 159/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0762 - acc: 0.9905 - val_loss: 0.0755 - val_acc: 0.9964\n",
      "Epoch 160/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0767 - acc: 0.9893 - val_loss: 0.0755 - val_acc: 0.9964\n",
      "Epoch 161/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0769 - acc: 0.9905 - val_loss: 0.0762 - val_acc: 0.9964\n",
      "Epoch 162/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0767 - acc: 0.9905 - val_loss: 0.0762 - val_acc: 0.9964\n",
      "Epoch 163/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0765 - acc: 0.9905 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 164/800\n",
      "842/842 [==============================] - 0s 372us/sample - loss: 0.0772 - acc: 0.9905 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 165/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0766 - acc: 0.9905 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 166/800\n",
      "842/842 [==============================] - 0s 368us/sample - loss: 0.0760 - acc: 0.9905 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 167/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0763 - acc: 0.9905 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 168/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0765 - acc: 0.9881 - val_loss: 0.0755 - val_acc: 0.9964\n",
      "Epoch 169/800\n",
      "842/842 [==============================] - 0s 358us/sample - loss: 0.0765 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 170/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0760 - acc: 0.9905 - val_loss: 0.0753 - val_acc: 0.9964\n",
      "Epoch 171/800\n",
      "842/842 [==============================] - 0s 470us/sample - loss: 0.0754 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 172/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0760 - acc: 0.9893 - val_loss: 0.0759 - val_acc: 0.9964\n",
      "Epoch 173/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.0767 - acc: 0.9905 - val_loss: 0.0753 - val_acc: 0.9964\n",
      "Epoch 174/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0762 - acc: 0.9893 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 175/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0763 - acc: 0.9905 - val_loss: 0.0755 - val_acc: 0.9964\n",
      "Epoch 176/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0758 - acc: 0.9905 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 177/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0758 - acc: 0.9905 - val_loss: 0.0767 - val_acc: 0.9964\n",
      "Epoch 178/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 0s 364us/sample - loss: 0.0764 - acc: 0.9905 - val_loss: 0.0750 - val_acc: 0.9964\n",
      "Epoch 179/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0752 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 180/800\n",
      "842/842 [==============================] - 0s 375us/sample - loss: 0.0762 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 181/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0761 - acc: 0.9905 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 182/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0761 - acc: 0.9905 - val_loss: 0.0775 - val_acc: 0.9964\n",
      "Epoch 183/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.0761 - acc: 0.9881 - val_loss: 0.0751 - val_acc: 0.9964\n",
      "Epoch 184/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0750 - acc: 0.9905 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 185/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0752 - acc: 0.9905 - val_loss: 0.0753 - val_acc: 0.9964\n",
      "Epoch 186/800\n",
      "842/842 [==============================] - 0s 367us/sample - loss: 0.0762 - acc: 0.9905 - val_loss: 0.0758 - val_acc: 0.9964\n",
      "Epoch 187/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0760 - acc: 0.9905 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 188/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0757 - acc: 0.9905 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 189/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0752 - acc: 0.9905 - val_loss: 0.0753 - val_acc: 0.9964\n",
      "Epoch 190/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0752 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 191/800\n",
      "842/842 [==============================] - 0s 341us/sample - loss: 0.0752 - acc: 0.9905 - val_loss: 0.0757 - val_acc: 0.9964\n",
      "Epoch 192/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0755 - acc: 0.9893 - val_loss: 0.0759 - val_acc: 0.9964\n",
      "Epoch 193/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0755 - acc: 0.9905 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 194/800\n",
      "842/842 [==============================] - 0s 358us/sample - loss: 0.0752 - acc: 0.9905 - val_loss: 0.0759 - val_acc: 0.9964\n",
      "Epoch 195/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0756 - acc: 0.9893 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 196/800\n",
      "842/842 [==============================] - 0s 391us/sample - loss: 0.0758 - acc: 0.9905 - val_loss: 0.0765 - val_acc: 0.9964\n",
      "Epoch 197/800\n",
      "842/842 [==============================] - 0s 375us/sample - loss: 0.0750 - acc: 0.9893 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 198/800\n",
      "842/842 [==============================] - 0s 380us/sample - loss: 0.0754 - acc: 0.9905 - val_loss: 0.0750 - val_acc: 0.9964\n",
      "Epoch 199/800\n",
      "842/842 [==============================] - 0s 407us/sample - loss: 0.0764 - acc: 0.9881 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 200/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0753 - acc: 0.9893 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 201/800\n",
      "842/842 [==============================] - 0s 378us/sample - loss: 0.0756 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 202/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0758 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 203/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0750 - acc: 0.9905 - val_loss: 0.0751 - val_acc: 0.9964\n",
      "Epoch 204/800\n",
      "842/842 [==============================] - 0s 368us/sample - loss: 0.0744 - acc: 0.9905 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 205/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0755 - acc: 0.9905 - val_loss: 0.0760 - val_acc: 0.9964\n",
      "Epoch 206/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0750 - acc: 0.9893 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 207/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0756 - acc: 0.9905 - val_loss: 0.0750 - val_acc: 0.9964\n",
      "Epoch 208/800\n",
      "842/842 [==============================] - 0s 397us/sample - loss: 0.0756 - acc: 0.9893 - val_loss: 0.0753 - val_acc: 0.9964\n",
      "Epoch 209/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0748 - acc: 0.9893 - val_loss: 0.0757 - val_acc: 0.9964\n",
      "Epoch 210/800\n",
      "842/842 [==============================] - 0s 393us/sample - loss: 0.0744 - acc: 0.9893 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 211/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0756 - acc: 0.9905 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 212/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0748 - acc: 0.9905 - val_loss: 0.0755 - val_acc: 0.9964\n",
      "Epoch 213/800\n",
      "842/842 [==============================] - 0s 378us/sample - loss: 0.0748 - acc: 0.9905 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 214/800\n",
      "842/842 [==============================] - 0s 361us/sample - loss: 0.0740 - acc: 0.9905 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 215/800\n",
      "842/842 [==============================] - 0s 384us/sample - loss: 0.0750 - acc: 0.9905 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 216/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0745 - acc: 0.9893 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 217/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0755 - acc: 0.9881 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 218/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0744 - acc: 0.9893 - val_loss: 0.0750 - val_acc: 0.9964\n",
      "Epoch 219/800\n",
      "842/842 [==============================] - 0s 418us/sample - loss: 0.0740 - acc: 0.9893 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 220/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0750 - acc: 0.9905 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 221/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0743 - acc: 0.9905 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 222/800\n",
      "842/842 [==============================] - 0s 373us/sample - loss: 0.0746 - acc: 0.9905 - val_loss: 0.0750 - val_acc: 0.9964\n",
      "Epoch 223/800\n",
      "842/842 [==============================] - 0s 376us/sample - loss: 0.0741 - acc: 0.9893 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 224/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0743 - acc: 0.9905 - val_loss: 0.0750 - val_acc: 0.9964\n",
      "Epoch 225/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0745 - acc: 0.9905 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 226/800\n",
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0740 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 227/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0746 - acc: 0.9905 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 228/800\n",
      "842/842 [==============================] - 0s 412us/sample - loss: 0.0752 - acc: 0.9881 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 229/800\n",
      "842/842 [==============================] - 0s 400us/sample - loss: 0.0740 - acc: 0.9893 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 230/800\n",
      "842/842 [==============================] - 0s 381us/sample - loss: 0.0741 - acc: 0.9881 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 231/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0740 - acc: 0.9905 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 232/800\n",
      "842/842 [==============================] - 0s 386us/sample - loss: 0.0745 - acc: 0.9893 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 233/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0742 - acc: 0.9881 - val_loss: 0.0750 - val_acc: 0.9964\n",
      "Epoch 234/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0750 - acc: 0.9905 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 235/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0740 - acc: 0.9905 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 236/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.0738 - acc: 0.9893 - val_loss: 0.0751 - val_acc: 0.9964\n",
      "Epoch 237/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 0s 385us/sample - loss: 0.0737 - acc: 0.9905 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 238/800\n",
      "842/842 [==============================] - 0s 375us/sample - loss: 0.0740 - acc: 0.9905 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 239/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0739 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 240/800\n",
      "842/842 [==============================] - 0s 367us/sample - loss: 0.0736 - acc: 0.9893 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 241/800\n",
      "842/842 [==============================] - 0s 411us/sample - loss: 0.0739 - acc: 0.9893 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 242/800\n",
      "842/842 [==============================] - 0s 414us/sample - loss: 0.0748 - acc: 0.9881 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 243/800\n",
      "842/842 [==============================] - 0s 424us/sample - loss: 0.0741 - acc: 0.9893 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 244/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0743 - acc: 0.9893 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 245/800\n",
      "842/842 [==============================] - 0s 363us/sample - loss: 0.0735 - acc: 0.9893 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 246/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0738 - acc: 0.9881 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 247/800\n",
      "842/842 [==============================] - 0s 382us/sample - loss: 0.0741 - acc: 0.9893 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 248/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0747 - acc: 0.9893 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 249/800\n",
      "842/842 [==============================] - 0s 375us/sample - loss: 0.0743 - acc: 0.9893 - val_loss: 0.0755 - val_acc: 0.9964\n",
      "Epoch 250/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0738 - acc: 0.9881 - val_loss: 0.0757 - val_acc: 0.9964\n",
      "Epoch 251/800\n",
      "842/842 [==============================] - 0s 511us/sample - loss: 0.0734 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 252/800\n",
      "842/842 [==============================] - 0s 362us/sample - loss: 0.0737 - acc: 0.9905 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 253/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0738 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 254/800\n",
      "842/842 [==============================] - 0s 358us/sample - loss: 0.0735 - acc: 0.9905 - val_loss: 0.0750 - val_acc: 0.9964\n",
      "Epoch 255/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0744 - acc: 0.9905 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 256/800\n",
      "842/842 [==============================] - 0s 394us/sample - loss: 0.0733 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 257/800\n",
      "842/842 [==============================] - 0s 416us/sample - loss: 0.0735 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 258/800\n",
      "842/842 [==============================] - 0s 385us/sample - loss: 0.0734 - acc: 0.9905 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 259/800\n",
      "842/842 [==============================] - 0s 361us/sample - loss: 0.0733 - acc: 0.9893 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 260/800\n",
      "842/842 [==============================] - 0s 386us/sample - loss: 0.0737 - acc: 0.9881 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 261/800\n",
      "842/842 [==============================] - 0s 374us/sample - loss: 0.0735 - acc: 0.9905 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 262/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0732 - acc: 0.9881 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 263/800\n",
      "842/842 [==============================] - 0s 397us/sample - loss: 0.0735 - acc: 0.9893 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 264/800\n",
      "842/842 [==============================] - 0s 449us/sample - loss: 0.0739 - acc: 0.9905 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 265/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0740 - acc: 0.9881 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 266/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0736 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 267/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0737 - acc: 0.9905 - val_loss: 0.0758 - val_acc: 0.9964\n",
      "Epoch 268/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0734 - acc: 0.9881 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 269/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0730 - acc: 0.9905 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 270/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0728 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 271/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0734 - acc: 0.9893 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 272/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0736 - acc: 0.9905 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 273/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0730 - acc: 0.9893 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 274/800\n",
      "842/842 [==============================] - 0s 342us/sample - loss: 0.0743 - acc: 0.9893 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 275/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0733 - acc: 0.9893 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 276/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0731 - acc: 0.9881 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 277/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0728 - acc: 0.9893 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 278/800\n",
      "842/842 [==============================] - 0s 441us/sample - loss: 0.0733 - acc: 0.9905 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 279/800\n",
      "842/842 [==============================] - 0s 405us/sample - loss: 0.0735 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 280/800\n",
      "842/842 [==============================] - 0s 395us/sample - loss: 0.0732 - acc: 0.9893 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 281/800\n",
      "842/842 [==============================] - 0s 343us/sample - loss: 0.0732 - acc: 0.9905 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 282/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0737 - acc: 0.9893 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 283/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0730 - acc: 0.9881 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 284/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0732 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 285/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0732 - acc: 0.9893 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 286/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0732 - acc: 0.9905 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 287/800\n",
      "842/842 [==============================] - 0s 358us/sample - loss: 0.0734 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 288/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0724 - acc: 0.9893 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 289/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0730 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 290/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0728 - acc: 0.9881 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 291/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0727 - acc: 0.9857 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 292/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0733 - acc: 0.9893 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 293/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0736 - acc: 0.9893 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 294/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0729 - acc: 0.9881 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 295/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0733 - acc: 0.9893 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 296/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0726 - acc: 0.9893 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 297/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0727 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 298/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0730 - acc: 0.9869 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 299/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0727 - acc: 0.9881 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 300/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0736 - acc: 0.9881 - val_loss: 0.0757 - val_acc: 0.9964\n",
      "Epoch 301/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0731 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 302/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0720 - acc: 0.9869 - val_loss: 0.0743 - val_acc: 0.9964\n",
      "Epoch 303/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0731 - acc: 0.9893 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 304/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0725 - acc: 0.9869 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 305/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0728 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 306/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0728 - acc: 0.9881 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 307/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0725 - acc: 0.9881 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 308/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0726 - acc: 0.9893 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 309/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0724 - acc: 0.9905 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 310/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0730 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 311/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0728 - acc: 0.9869 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 312/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0720 - acc: 0.9881 - val_loss: 0.0751 - val_acc: 0.9964\n",
      "Epoch 313/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0715 - acc: 0.9905 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 314/800\n",
      "842/842 [==============================] - 0s 363us/sample - loss: 0.0726 - acc: 0.9893 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 315/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0727 - acc: 0.9893 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 316/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0725 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 317/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0717 - acc: 0.9893 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 318/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0721 - acc: 0.9893 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 319/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0722 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 320/800\n",
      "842/842 [==============================] - 0s 367us/sample - loss: 0.0735 - acc: 0.9869 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 321/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0723 - acc: 0.9893 - val_loss: 0.0749 - val_acc: 0.9964\n",
      "Epoch 322/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0725 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 323/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0726 - acc: 0.9893 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 324/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0727 - acc: 0.9905 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 325/800\n",
      "842/842 [==============================] - 0s 367us/sample - loss: 0.0738 - acc: 0.9905 - val_loss: 0.0746 - val_acc: 0.9964\n",
      "Epoch 326/800\n",
      "842/842 [==============================] - 0s 342us/sample - loss: 0.0721 - acc: 0.9881 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 327/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0718 - acc: 0.9893 - val_loss: 0.0739 - val_acc: 0.9964\n",
      "Epoch 328/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0726 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 329/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0733 - acc: 0.9893 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 330/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0728 - acc: 0.9893 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 331/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0726 - acc: 0.9881 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 332/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0728 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 333/800\n",
      "842/842 [==============================] - 0s 344us/sample - loss: 0.0718 - acc: 0.9893 - val_loss: 0.0739 - val_acc: 0.9964\n",
      "Epoch 334/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0730 - acc: 0.9893 - val_loss: 0.0759 - val_acc: 0.9964\n",
      "Epoch 335/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0726 - acc: 0.9846 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 336/800\n",
      "842/842 [==============================] - 0s 385us/sample - loss: 0.0730 - acc: 0.9869 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 337/800\n",
      "842/842 [==============================] - 0s 361us/sample - loss: 0.0726 - acc: 0.9893 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 338/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0716 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 339/800\n",
      "842/842 [==============================] - 0s 343us/sample - loss: 0.0727 - acc: 0.9846 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 340/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0720 - acc: 0.9881 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 341/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0718 - acc: 0.9881 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 342/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0720 - acc: 0.9893 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 343/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0721 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 344/800\n",
      "842/842 [==============================] - 0s 351us/sample - loss: 0.0716 - acc: 0.9893 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 345/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0715 - acc: 0.9869 - val_loss: 0.0754 - val_acc: 0.9964\n",
      "Epoch 346/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0718 - acc: 0.9893 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 347/800\n",
      "842/842 [==============================] - 0s 342us/sample - loss: 0.0723 - acc: 0.9881 - val_loss: 0.0752 - val_acc: 0.9964\n",
      "Epoch 348/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0725 - acc: 0.9881 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 349/800\n",
      "842/842 [==============================] - 0s 344us/sample - loss: 0.0716 - acc: 0.9869 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 350/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0733 - acc: 0.9893 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 351/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0718 - acc: 0.9881 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 352/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0722 - acc: 0.9893 - val_loss: 0.0739 - val_acc: 0.9964\n",
      "Epoch 353/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0724 - acc: 0.9881 - val_loss: 0.0738 - val_acc: 0.9964\n",
      "Epoch 354/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0718 - acc: 0.9881 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 355/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0725 - acc: 0.9905 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 356/800\n",
      "842/842 [==============================] - 0s 368us/sample - loss: 0.0718 - acc: 0.9881 - val_loss: 0.0756 - val_acc: 0.9964\n",
      "Epoch 357/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0717 - acc: 0.9881 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 358/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0721 - acc: 0.9869 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 359/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0717 - acc: 0.9869 - val_loss: 0.0747 - val_acc: 0.9964\n",
      "Epoch 360/800\n",
      "842/842 [==============================] - 0s 392us/sample - loss: 0.0723 - acc: 0.9881 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 361/800\n",
      "842/842 [==============================] - 0s 398us/sample - loss: 0.0718 - acc: 0.9893 - val_loss: 0.0734 - val_acc: 0.9964\n",
      "Epoch 362/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0724 - acc: 0.9893 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 363/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0720 - acc: 0.9881 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 364/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0717 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 365/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0726 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9964\n",
      "Epoch 366/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0717 - acc: 0.9893 - val_loss: 0.0745 - val_acc: 0.9964\n",
      "Epoch 367/800\n",
      "842/842 [==============================] - 0s 344us/sample - loss: 0.0718 - acc: 0.9881 - val_loss: 0.0755 - val_acc: 0.9964\n",
      "Epoch 368/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0716 - acc: 0.9881 - val_loss: 0.0738 - val_acc: 0.9964\n",
      "Epoch 369/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0721 - acc: 0.9846 - val_loss: 0.0748 - val_acc: 0.9964\n",
      "Epoch 370/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0721 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 371/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0712 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 372/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0717 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 373/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0719 - acc: 0.9893 - val_loss: 0.0739 - val_acc: 0.9964\n",
      "Epoch 374/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0711 - acc: 0.9857 - val_loss: 0.0737 - val_acc: 0.9964\n",
      "Epoch 375/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0714 - acc: 0.9905 - val_loss: 0.0739 - val_acc: 0.9964\n",
      "Epoch 376/800\n",
      "842/842 [==============================] - 0s 344us/sample - loss: 0.0724 - acc: 0.9869 - val_loss: 0.0744 - val_acc: 0.9964\n",
      "Epoch 377/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0714 - acc: 0.9869 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 378/800\n",
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0709 - acc: 0.9869 - val_loss: 0.0737 - val_acc: 0.9964\n",
      "Epoch 379/800\n",
      "842/842 [==============================] - 0s 344us/sample - loss: 0.0717 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9964\n",
      "Epoch 380/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0718 - acc: 0.9905 - val_loss: 0.0739 - val_acc: 0.9964\n",
      "Epoch 381/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0718 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9964\n",
      "Epoch 382/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0716 - acc: 0.9857 - val_loss: 0.0753 - val_acc: 0.9964\n",
      "Epoch 383/800\n",
      "842/842 [==============================] - 0s 342us/sample - loss: 0.0711 - acc: 0.9869 - val_loss: 0.0740 - val_acc: 0.9964\n",
      "Epoch 384/800\n",
      "842/842 [==============================] - 0s 344us/sample - loss: 0.0714 - acc: 0.9893 - val_loss: 0.0738 - val_acc: 0.9964\n",
      "Epoch 385/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0712 - acc: 0.9881 - val_loss: 0.0748 - val_acc: 0.9929\n",
      "Epoch 386/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0709 - acc: 0.9846 - val_loss: 0.0736 - val_acc: 0.9929\n",
      "Epoch 387/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0716 - acc: 0.9869 - val_loss: 0.0737 - val_acc: 0.9929\n",
      "Epoch 388/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0714 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9929\n",
      "Epoch 389/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0716 - acc: 0.9881 - val_loss: 0.0747 - val_acc: 0.9929\n",
      "Epoch 390/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0716 - acc: 0.9869 - val_loss: 0.0739 - val_acc: 0.9929\n",
      "Epoch 391/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0711 - acc: 0.9881 - val_loss: 0.0749 - val_acc: 0.9929\n",
      "Epoch 392/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0716 - acc: 0.9881 - val_loss: 0.0736 - val_acc: 0.9929\n",
      "Epoch 393/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0710 - acc: 0.9881 - val_loss: 0.0742 - val_acc: 0.9929\n",
      "Epoch 394/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0719 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9929\n",
      "Epoch 395/800\n",
      "842/842 [==============================] - 0s 429us/sample - loss: 0.0714 - acc: 0.9893 - val_loss: 0.0740 - val_acc: 0.9929\n",
      "Epoch 396/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0719 - acc: 0.9881 - val_loss: 0.0739 - val_acc: 0.9929\n",
      "Epoch 397/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0713 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9929\n",
      "Epoch 398/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0713 - acc: 0.9881 - val_loss: 0.0738 - val_acc: 0.9929\n",
      "Epoch 399/800\n",
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0721 - acc: 0.9881 - val_loss: 0.0738 - val_acc: 0.9929\n",
      "Epoch 400/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0716 - acc: 0.9857 - val_loss: 0.0735 - val_acc: 0.9929\n",
      "Epoch 401/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0715 - acc: 0.9869 - val_loss: 0.0743 - val_acc: 0.9929\n",
      "Epoch 402/800\n",
      "842/842 [==============================] - 0s 404us/sample - loss: 0.0711 - acc: 0.9857 - val_loss: 0.0739 - val_acc: 0.9929\n",
      "Epoch 403/800\n",
      "842/842 [==============================] - 0s 341us/sample - loss: 0.0716 - acc: 0.9881 - val_loss: 0.0739 - val_acc: 0.9929\n",
      "Epoch 404/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0707 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9929\n",
      "Epoch 405/800\n",
      "842/842 [==============================] - 0s 345us/sample - loss: 0.0721 - acc: 0.9869 - val_loss: 0.0741 - val_acc: 0.9929\n",
      "Epoch 406/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0717 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9929\n",
      "Epoch 407/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0715 - acc: 0.9869 - val_loss: 0.0737 - val_acc: 0.9929\n",
      "Epoch 408/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0710 - acc: 0.9881 - val_loss: 0.0737 - val_acc: 0.9929\n",
      "Epoch 409/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0717 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9929\n",
      "Epoch 410/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0707 - acc: 0.9869 - val_loss: 0.0740 - val_acc: 0.9929\n",
      "Epoch 411/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0705 - acc: 0.9869 - val_loss: 0.0736 - val_acc: 0.9929\n",
      "Epoch 412/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0710 - acc: 0.9869 - val_loss: 0.0735 - val_acc: 0.9929\n",
      "Epoch 413/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0713 - acc: 0.9893 - val_loss: 0.0742 - val_acc: 0.9929\n",
      "Epoch 414/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0713 - acc: 0.9857 - val_loss: 0.0744 - val_acc: 0.9929\n",
      "Epoch 415/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0708 - acc: 0.9893 - val_loss: 0.0735 - val_acc: 0.9929\n",
      "Epoch 416/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0704 - acc: 0.9869 - val_loss: 0.0738 - val_acc: 0.9929\n",
      "Epoch 417/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0714 - acc: 0.9869 - val_loss: 0.0738 - val_acc: 0.9929\n",
      "Epoch 418/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0705 - acc: 0.9869 - val_loss: 0.0745 - val_acc: 0.9929\n",
      "Epoch 419/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0706 - acc: 0.9893 - val_loss: 0.0737 - val_acc: 0.9929\n",
      "Epoch 420/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0714 - acc: 0.9869 - val_loss: 0.0741 - val_acc: 0.9929\n",
      "Epoch 421/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0709 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 422/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0715 - acc: 0.9846 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 423/800\n",
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0713 - acc: 0.9881 - val_loss: 0.0730 - val_acc: 0.9893\n",
      "Epoch 424/800\n",
      "842/842 [==============================] - 0s 363us/sample - loss: 0.0711 - acc: 0.9881 - val_loss: 0.0742 - val_acc: 0.9893\n",
      "Epoch 425/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0721 - acc: 0.9881 - val_loss: 0.0740 - val_acc: 0.9893\n",
      "Epoch 426/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0708 - acc: 0.9869 - val_loss: 0.0746 - val_acc: 0.9893\n",
      "Epoch 427/800\n",
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0712 - acc: 0.9881 - val_loss: 0.0746 - val_acc: 0.9893\n",
      "Epoch 428/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0707 - acc: 0.9857 - val_loss: 0.0740 - val_acc: 0.9893\n",
      "Epoch 429/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0707 - acc: 0.9857 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 430/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0709 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 431/800\n",
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0713 - acc: 0.9857 - val_loss: 0.0745 - val_acc: 0.9893\n",
      "Epoch 432/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0707 - acc: 0.9869 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 433/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0716 - acc: 0.9857 - val_loss: 0.0735 - val_acc: 0.9893\n",
      "Epoch 434/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0715 - acc: 0.9869 - val_loss: 0.0735 - val_acc: 0.9893\n",
      "Epoch 435/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0712 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9893\n",
      "Epoch 436/800\n",
      "842/842 [==============================] - 0s 359us/sample - loss: 0.0720 - acc: 0.9869 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 437/800\n",
      "842/842 [==============================] - 0s 363us/sample - loss: 0.0710 - acc: 0.9869 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 438/800\n",
      "842/842 [==============================] - 0s 435us/sample - loss: 0.0711 - acc: 0.9893 - val_loss: 0.0744 - val_acc: 0.9893\n",
      "Epoch 439/800\n",
      "842/842 [==============================] - 0s 500us/sample - loss: 0.0717 - acc: 0.9869 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 440/800\n",
      "842/842 [==============================] - 0s 467us/sample - loss: 0.0709 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9893\n",
      "Epoch 441/800\n",
      "842/842 [==============================] - 0s 477us/sample - loss: 0.0707 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 442/800\n",
      "842/842 [==============================] - 0s 403us/sample - loss: 0.0705 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 443/800\n",
      "842/842 [==============================] - 0s 409us/sample - loss: 0.0708 - acc: 0.9857 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 444/800\n",
      "842/842 [==============================] - 0s 463us/sample - loss: 0.0708 - acc: 0.9857 - val_loss: 0.0743 - val_acc: 0.9893\n",
      "Epoch 445/800\n",
      "842/842 [==============================] - 0s 416us/sample - loss: 0.0707 - acc: 0.9869 - val_loss: 0.0736 - val_acc: 0.9893\n",
      "Epoch 446/800\n",
      "842/842 [==============================] - 0s 449us/sample - loss: 0.0704 - acc: 0.9869 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 447/800\n",
      "842/842 [==============================] - 0s 545us/sample - loss: 0.0708 - acc: 0.9846 - val_loss: 0.0742 - val_acc: 0.9893\n",
      "Epoch 448/800\n",
      "842/842 [==============================] - 0s 445us/sample - loss: 0.0720 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 449/800\n",
      "842/842 [==============================] - 0s 534us/sample - loss: 0.0708 - acc: 0.9881 - val_loss: 0.0740 - val_acc: 0.9893\n",
      "Epoch 450/800\n",
      "842/842 [==============================] - 0s 481us/sample - loss: 0.0704 - acc: 0.9857 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 451/800\n",
      "842/842 [==============================] - 0s 410us/sample - loss: 0.0703 - acc: 0.9869 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 452/800\n",
      "842/842 [==============================] - 0s 400us/sample - loss: 0.0708 - acc: 0.9846 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 453/800\n",
      "842/842 [==============================] - 0s 419us/sample - loss: 0.0709 - acc: 0.9857 - val_loss: 0.0736 - val_acc: 0.9893\n",
      "Epoch 454/800\n",
      "842/842 [==============================] - 0s 410us/sample - loss: 0.0701 - acc: 0.9881 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 455/800\n",
      "842/842 [==============================] - 0s 436us/sample - loss: 0.0709 - acc: 0.9869 - val_loss: 0.0734 - val_acc: 0.9893\n",
      "Epoch 456/800\n",
      "842/842 [==============================] - 0s 436us/sample - loss: 0.0707 - acc: 0.9869 - val_loss: 0.0745 - val_acc: 0.9893\n",
      "Epoch 457/800\n",
      "842/842 [==============================] - 0s 429us/sample - loss: 0.0704 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 458/800\n",
      "842/842 [==============================] - 0s 387us/sample - loss: 0.0703 - acc: 0.9869 - val_loss: 0.0740 - val_acc: 0.9893\n",
      "Epoch 459/800\n",
      "842/842 [==============================] - 0s 394us/sample - loss: 0.0713 - acc: 0.9881 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 460/800\n",
      "842/842 [==============================] - 0s 398us/sample - loss: 0.0705 - acc: 0.9857 - val_loss: 0.0736 - val_acc: 0.9893\n",
      "Epoch 461/800\n",
      "842/842 [==============================] - 0s 393us/sample - loss: 0.0710 - acc: 0.9857 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 462/800\n",
      "842/842 [==============================] - 0s 390us/sample - loss: 0.0704 - acc: 0.9893 - val_loss: 0.0746 - val_acc: 0.9893\n",
      "Epoch 463/800\n",
      "842/842 [==============================] - 0s 395us/sample - loss: 0.0705 - acc: 0.9869 - val_loss: 0.0744 - val_acc: 0.9893\n",
      "Epoch 464/800\n",
      "842/842 [==============================] - 0s 405us/sample - loss: 0.0705 - acc: 0.9857 - val_loss: 0.0740 - val_acc: 0.9893\n",
      "Epoch 465/800\n",
      "842/842 [==============================] - 0s 413us/sample - loss: 0.0714 - acc: 0.9893 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 466/800\n",
      "842/842 [==============================] - 0s 394us/sample - loss: 0.0702 - acc: 0.9857 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 467/800\n",
      "842/842 [==============================] - 0s 400us/sample - loss: 0.0712 - acc: 0.9857 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 468/800\n",
      "842/842 [==============================] - 0s 415us/sample - loss: 0.0710 - acc: 0.9834 - val_loss: 0.0750 - val_acc: 0.9893\n",
      "Epoch 469/800\n",
      "842/842 [==============================] - 0s 400us/sample - loss: 0.0710 - acc: 0.9869 - val_loss: 0.0749 - val_acc: 0.9893\n",
      "Epoch 470/800\n",
      "842/842 [==============================] - 0s 487us/sample - loss: 0.0702 - acc: 0.9869 - val_loss: 0.0748 - val_acc: 0.9893\n",
      "Epoch 471/800\n",
      "842/842 [==============================] - 0s 413us/sample - loss: 0.0707 - acc: 0.9846 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 472/800\n",
      "842/842 [==============================] - 0s 396us/sample - loss: 0.0709 - acc: 0.9869 - val_loss: 0.0732 - val_acc: 0.9893\n",
      "Epoch 473/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 0s 412us/sample - loss: 0.0703 - acc: 0.9857 - val_loss: 0.0740 - val_acc: 0.9893\n",
      "Epoch 474/800\n",
      "842/842 [==============================] - 0s 394us/sample - loss: 0.0707 - acc: 0.9869 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 475/800\n",
      "842/842 [==============================] - 0s 396us/sample - loss: 0.0698 - acc: 0.9822 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 476/800\n",
      "842/842 [==============================] - 0s 393us/sample - loss: 0.0699 - acc: 0.9881 - val_loss: 0.0735 - val_acc: 0.9893\n",
      "Epoch 477/800\n",
      "842/842 [==============================] - 0s 362us/sample - loss: 0.0705 - acc: 0.9869 - val_loss: 0.0749 - val_acc: 0.9893\n",
      "Epoch 478/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0705 - acc: 0.9869 - val_loss: 0.0742 - val_acc: 0.9893\n",
      "Epoch 479/800\n",
      "842/842 [==============================] - 0s 367us/sample - loss: 0.0705 - acc: 0.9893 - val_loss: 0.0736 - val_acc: 0.9893\n",
      "Epoch 480/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0713 - acc: 0.9846 - val_loss: 0.0745 - val_acc: 0.9893\n",
      "Epoch 481/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0705 - acc: 0.9869 - val_loss: 0.0736 - val_acc: 0.9893\n",
      "Epoch 482/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0705 - acc: 0.9869 - val_loss: 0.0750 - val_acc: 0.9893\n",
      "Epoch 483/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0705 - acc: 0.9846 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 484/800\n",
      "842/842 [==============================] - 0s 344us/sample - loss: 0.0705 - acc: 0.9846 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 485/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0703 - acc: 0.9881 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 486/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0707 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 487/800\n",
      "842/842 [==============================] - 0s 360us/sample - loss: 0.0710 - acc: 0.9857 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 488/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0702 - acc: 0.9846 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 489/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0709 - acc: 0.9881 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 490/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0702 - acc: 0.9834 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 491/800\n",
      "842/842 [==============================] - 0s 348us/sample - loss: 0.0708 - acc: 0.9881 - val_loss: 0.0744 - val_acc: 0.9893\n",
      "Epoch 492/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0697 - acc: 0.9881 - val_loss: 0.0750 - val_acc: 0.9893\n",
      "Epoch 493/800\n",
      "842/842 [==============================] - 0s 363us/sample - loss: 0.0712 - acc: 0.9869 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 494/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0706 - acc: 0.9881 - val_loss: 0.0735 - val_acc: 0.9893\n",
      "Epoch 495/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0705 - acc: 0.9846 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 496/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0701 - acc: 0.9869 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 497/800\n",
      "842/842 [==============================] - 0s 369us/sample - loss: 0.0700 - acc: 0.9893 - val_loss: 0.0742 - val_acc: 0.9893\n",
      "Epoch 498/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0703 - acc: 0.9857 - val_loss: 0.0735 - val_acc: 0.9893\n",
      "Epoch 499/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0702 - acc: 0.9857 - val_loss: 0.0734 - val_acc: 0.9893\n",
      "Epoch 500/800\n",
      "842/842 [==============================] - 0s 371us/sample - loss: 0.0704 - acc: 0.9857 - val_loss: 0.0736 - val_acc: 0.9893\n",
      "Epoch 501/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0707 - acc: 0.9846 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 502/800\n",
      "842/842 [==============================] - 0s 357us/sample - loss: 0.0706 - acc: 0.9881 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 503/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0702 - acc: 0.9881 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 504/800\n",
      "842/842 [==============================] - 0s 343us/sample - loss: 0.0707 - acc: 0.9857 - val_loss: 0.0746 - val_acc: 0.9893\n",
      "Epoch 505/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0702 - acc: 0.9869 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 506/800\n",
      "842/842 [==============================] - 0s 356us/sample - loss: 0.0697 - acc: 0.9869 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 507/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0707 - acc: 0.9869 - val_loss: 0.0743 - val_acc: 0.9893\n",
      "Epoch 508/800\n",
      "842/842 [==============================] - 0s 342us/sample - loss: 0.0693 - acc: 0.9857 - val_loss: 0.0746 - val_acc: 0.9893\n",
      "Epoch 509/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0697 - acc: 0.9822 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 510/800\n",
      "842/842 [==============================] - 0s 346us/sample - loss: 0.0709 - acc: 0.9857 - val_loss: 0.0738 - val_acc: 0.9893\n",
      "Epoch 511/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0698 - acc: 0.9881 - val_loss: 0.0740 - val_acc: 0.9893\n",
      "Epoch 512/800\n",
      "842/842 [==============================] - 0s 350us/sample - loss: 0.0718 - acc: 0.9857 - val_loss: 0.0744 - val_acc: 0.9893\n",
      "Epoch 513/800\n",
      "842/842 [==============================] - 0s 352us/sample - loss: 0.0706 - acc: 0.9869 - val_loss: 0.0735 - val_acc: 0.9893\n",
      "Epoch 514/800\n",
      "842/842 [==============================] - 0s 347us/sample - loss: 0.0703 - acc: 0.9857 - val_loss: 0.0745 - val_acc: 0.9893\n",
      "Epoch 515/800\n",
      "842/842 [==============================] - 0s 366us/sample - loss: 0.0701 - acc: 0.9869 - val_loss: 0.0736 - val_acc: 0.9893\n",
      "Epoch 516/800\n",
      "842/842 [==============================] - 0s 363us/sample - loss: 0.0698 - acc: 0.9846 - val_loss: 0.0742 - val_acc: 0.9893\n",
      "Epoch 517/800\n",
      "842/842 [==============================] - 0s 349us/sample - loss: 0.0698 - acc: 0.9881 - val_loss: 0.0750 - val_acc: 0.9893\n",
      "Epoch 518/800\n",
      "842/842 [==============================] - 0s 353us/sample - loss: 0.0696 - acc: 0.9893 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 519/800\n",
      "842/842 [==============================] - 0s 365us/sample - loss: 0.0706 - acc: 0.9846 - val_loss: 0.0737 - val_acc: 0.9893\n",
      "Epoch 520/800\n",
      "842/842 [==============================] - 0s 355us/sample - loss: 0.0694 - acc: 0.9869 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 521/800\n",
      "842/842 [==============================] - 0s 354us/sample - loss: 0.0701 - acc: 0.9869 - val_loss: 0.0739 - val_acc: 0.9893\n",
      "Epoch 522/800\n",
      "842/842 [==============================] - 0s 368us/sample - loss: 0.0695 - acc: 0.9869 - val_loss: 0.0741 - val_acc: 0.9893\n",
      "Epoch 523/800\n",
      "842/842 [==============================] - 0s 481us/sample - loss: 0.0707 - acc: 0.9881 - val_loss: 0.0738 - val_acc: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.5_AUC</th>\n",
       "      <th>0.5_F1</th>\n",
       "      <th>0.5_MCC</th>\n",
       "      <th>0.5_AUC</th>\n",
       "      <th>0.5_F1</th>\n",
       "      <th>0.5_MCC</th>\n",
       "      <th>custom_AUC</th>\n",
       "      <th>custom_F1</th>\n",
       "      <th>custom_MCC</th>\n",
       "      <th>custom_AUC</th>\n",
       "      <th>custom_F1</th>\n",
       "      <th>custom_MCC</th>\n",
       "      <th>FRS_Threshold</th>\n",
       "      <th>CHEL_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <td>0.798759</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>0.368402</td>\n",
       "      <td>0.635052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.798759</td>\n",
       "      <td>0.738007</td>\n",
       "      <td>0.507973</td>\n",
       "      <td>0.635052</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.223788</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.5_AUC    0.5_F1   0.5_MCC   0.5_AUC  0.5_F1  0.5_MCC  custom_AUC  \\\n",
       "metrics  0.798759  0.495146  0.368402  0.635052     0.0      0.0    0.798759   \n",
       "\n",
       "         custom_F1  custom_MCC  custom_AUC  custom_F1  custom_MCC  \\\n",
       "metrics   0.738007    0.507973    0.635052   0.298507    0.223788   \n",
       "\n",
       "         FRS_Threshold  CHEL_Threshold  \n",
       "metrics           0.44            0.27  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VPW9//HXZ7ZM9h0ICZCwyB4ChEVRxKUWtKJWVKxW0Vpa7WJ72/vT1t7a2uut2l5rba3VttqrdSnFqmhxr7gBStgCYZEtkJAA2fdtZr6/P84EQhYyQGDCzOf5eOSRmbPN9zuE9/me7znne8QYg1JKqfBgC3YBlFJKnT4a+kopFUY09JVSKoxo6CulVBjR0FdKqTCioa+UUmFEQ18ppcKIhr5SSoURDX2llAojjmAXoLOUlBSTmZkZ7GIopdQZZe3ateXGmNTelut3oZ+ZmUleXl6wi6GUUmcUEdkbyHLavaOUUmFEQ18ppcKIhr5SSoWRftenr5QKLW1tbRQXF9Pc3BzsooQEt9tNRkYGTqfzhNbX0FdKnVLFxcXExsaSmZmJiAS7OGc0YwwVFRUUFxeTlZV1QtvQ7h2l1CnV3NxMcnKyBn4fEBGSk5NP6qhJQ18pdcpp4Pedk/0uQyb065rb+M07n7OhqDrYRVFKqX4rZELf6zP89r0drNtbFeyiKKX6kerqav7whz8c93qXXnop1dWh14gMmdCPjrDOSde3eIJcEqVUf9JT6Hu93mOut3z5chISEk5VsYImZK7ecdptRDrtGvpKqaPcfffd7Nq1i5ycHJxOJzExMaSlpbFhwwa2bNnClVdeSVFREc3Nzdx5550sXrwYODIkTH19PfPmzePcc89l5cqVpKen8+qrrxIZGRnkmp2YkAl9gBi3g7rmtmAXQynVg5+/VsCWkto+3ea4wXHce/n4Huc/8MADbN68mQ0bNrBixQouu+wyNm/efPiSx6eeeoqkpCSampqYNm0aV199NcnJyUdtY8eOHbzwwgv86U9/4tprr+Wll17ixhtv7NN6nC4hFfqxEQ7qmrWlr5Tq2fTp04+6xv3RRx/l5ZdfBqCoqIgdO3Z0Cf2srCxycnIAmDp1KoWFhaetvH0ttELf7dDuHaX6sWO1yE+X6Ojow69XrFjBu+++y6pVq4iKimLOnDndXgMfERFx+LXdbqepqem0lPVUCJkTudDevaOhr5Q6IjY2lrq6um7n1dTUkJiYSFRUFNu2bWP16tWnuXSnX0i19GMiHJTXNQa7GEqpfiQ5OZlZs2YxYcIEIiMjGThw4OF5c+fO5Y9//CPZ2dmMHj2amTNnBrGkp0dAoS8ic4HfAnbgz8aYBzrN/w/gNsADlAG3GmP2+ufdDPzEv+h/G2P+r4/K3kWs26ndO0qpLp5//vlup0dERPDGG290O6+93z4lJYXNmzcfnv7DH/6wz8t3OvXavSMiduAxYB4wDrheRMZ1Wmw9kGuMyQaWAg/5100C7gVmANOBe0Ukse+Kf7SYCAe1evWOUkr1KJA+/enATmPMbmNMK/AicEXHBYwx7xtj2vtVVgMZ/tdfBN4xxlQaY6qAd4C5fVP0rtpP5BpjTtVHKKXUGS2Q0E8Hijq8L/ZP68nXgPbjpeNd96RERzgwBprajn2nnVJKhatA+vS7G9Kt26a0iNwI5ALnH8+6IrIYWAwwdOjQAIrUvfahGBpavES5QuoctVJK9YlAWvrFwJAO7zOAks4LicjFwD3AfGNMy/Gsa4x50hiTa4zJTU1NDbTsXUS77AA0turJXKWU6k4gob8GGCUiWSLiAhYCyzouICKTgSewAv9Qh1lvAZeISKL/BO4l/mmnRHvrXq/gUUqp7vUa+sYYD/BtrLDeCiwxxhSIyH0iMt+/2K+AGOAfIrJBRJb5160EfoG141gD3OefdkrE+Lt3Glu1T18pdWJiYmIAKCkpYcGCBd0uM2fOHPLy8o65nUceeYTGxiP3DfWXoZoD6vg2xiwHlnea9tMOry8+xrpPAU+daAGPR1SE1b3ToC19pdRJGjx4MEuXLj3h9R955BFuvPFGoqKiAGuo5v4gpIZhiHYdOZGrlFIAd91111Hj6f/sZz/j5z//ORdddBFTpkxh4sSJvPrqq13WKywsZMKECQA0NTWxcOFCsrOzue66644ae+f2228nNzeX8ePHc++99wLWIG4lJSVccMEFXHDBBYA1VHN5eTkADz/8MBMmTGDChAk88sgjhz9v7NixfP3rX2f8+PFccsklp2SMn5C6xCW6vaWvJ3KV6p/euBsObOrbbQ6aCPMe6HH2woUL+d73vscdd9wBwJIlS3jzzTf5/ve/T1xcHOXl5cycOZP58+f3+PzZxx9/nKioKPLz88nPz2fKlCmH591///0kJSXh9Xq56KKLyM/P57vf/S4PP/ww77//PikpKUdta+3atTz99NN8+umnGGOYMWMG559/PomJiadlCOcQbelr6CulLJMnT+bQoUOUlJSwceNGEhMTSUtL48c//jHZ2dlcfPHF7N+/n4MHD/a4jQ8//PBw+GZnZ5OdnX143pIlS5gyZQqTJ0+moKCALVu2HLM8H3/8MVdddRXR0dHExMTw5S9/mY8++gg4PUM4h1RLv71PX0/kKtVPHaNFfiotWLCApUuXcuDAARYuXMhzzz1HWVkZa9euxel0kpmZ2e2Qyh11dxSwZ88efv3rX7NmzRoSExNZtGhRr9s51ogBp2MI55Bq6Uc47Djtoi19pdRRFi5cyIsvvsjSpUtZsGABNTU1DBgwAKfTyfvvv8/evXuPuf7s2bN57rnnANi8eTP5+fkA1NbWEh0dTXx8PAcPHjxq8LaehnSePXs2r7zyCo2NjTQ0NPDyyy9z3nnn9WFtjy2kWvpgXauvoa+U6mj8+PHU1dWRnp5OWloaN9xwA5dffjm5ubnk5OQwZsyYY65/++23c8stt5CdnU1OTg7Tp08HYNKkSUyePJnx48czfPhwZs2adXidxYsXM2/ePNLS0nj//fcPT58yZQqLFi06vI3bbruNyZMnn7ancUl/G5wsNzfX9Hb967Gc88v3OGdkCr++ZlIflkopdaK2bt3K2LFjg12MkNLddyoia40xub2tG1LdO2CNv6MtfaWU6l7IhX5UhIMGPZGrlFLdCrnQj3bZadSWvlL9Sn/rRj6Tnex3GXqhH+HQAdeU6kfcbjcVFRUa/H3AGENFRQVut/uEtxFyV+9Eu+x6nb5S/UhGRgbFxcWUlZUFuyghwe12k5GR0fuCPQi50I+KcOh4+kr1I06nk6ysrGAXQ/mFXPdOjHbvKKVUj0Iu9KNcdprbfHh92n+olFKdhVzoHx50Tbt4lFKqi9AJ/aYqWHorI2o/A6BRx9RXSqkuQif0ATa/RGrzHkCfk6uUUt0JndB3RgMQJS2AjqmvlFLdCZ3Qd7jA5sRtrPGntaWvlFJdBRT6IjJXRLaLyE4Rubub+bNFZJ2IeERkQad5D4lIgYhsFZFHpafnkfUFVzRun/UAAw19pZTqqtfQFxE78BgwDxgHXC8i4zottg9YBDzfad1zgFlANjABmAacf9Kl7okrhoj2ln6zhr5SSnUWyB2504GdxpjdACLyInAFcPhBkMaYQv88X6d1DeAGXIAATqDnB1GeLFcUTq927yilVE8C6d5JB4o6vC/2T+uVMWYV8D5Q6v95yxiztfNyIrJYRPJEJO+kxudwRePwNgIa+kop1Z1AQr+7PviAbncVkZHAWCADa0dxoYjM7rIxY540xuQaY3JTU1MD2XT3XDHY2xpx2W3UafeOUkp1EUjoFwNDOrzPAEoC3P5VwGpjTL0xph54A5h5fEU8Dq5oaGsgOsKul2wqpVQ3Agn9NcAoEckSERewEFgW4Pb3AeeLiENEnFgncbt07/QZVzS0NhDj1kHXlFKqO72GvjHGA3wbeAsrsJcYYwpE5D4RmQ8gItNEpBi4BnhCRAr8qy8FdgGbgI3ARmPMa6egHhZnlBX6EU7t3lFKqW4ENJ6+MWY5sLzTtJ92eL0Gq9un83pe4BsnWcbAuWKgtZHYaAf1LW2n7WOVUupMETp35IK/e6eemAi7du8opVQ3Qiv0I2LAeEl0eWnQUTaVUqqL0HpcYkQcAMmOZuqaNfSVUqqz0Ap9dzxghX59S5DLopRS/VCIde9YLf0EWzPNbT7avJ1HhVBKqfAWWqHvtkI/3mYNxaA3aCml1NFCK/T9Lf040UHXlFKqO6EV+v6Wfiw66JpSSnUntELf39KPMg2AjqmvlFKdhVbou2IAIcpYLf06bekrpdRRQiv0bTZwxxHddABAx99RSqlOQiv0AcZeTuyOf5JKNbVNOv6OUkp1FHqhP/pSxOdhgFRRo6GvlFJHCb3Qd0YCEG9v05a+Ukp1Enqh77BCPynCpy19pZTqJPRC39/ST3J5NPSVUqqTkA39RJdXQ18ppToJ2dCPd2hLXymlOgu90Pf36cc5tKWvlFKdBRT6IjJXRLaLyE4Rubub+bNFZJ2IeERkQad5Q0XkbRHZKiJbRCSzb4reA2d76Ldp6CulVCe9hr6I2IHHgHnAOOB6ERnXabF9wCLg+W428QzwK2PMWGA6cOhkCtwrf+jH2Nqoa/bg9ZlT+nFKKXUmCaSlPx3YaYzZbYxpBV4Erui4gDGm0BiTDxz11BL/zsFhjHnHv1y9Mf6BcU4Vmx1sTqJtViu/rllb+0op1S6Q0E8Hijq8L/ZPC8RZQLWI/FNE1ovIr/xHDqeWM4oof+hrF49SSh0RSOhLN9MC7TNxAOcBPwSmAcOxuoGO/gCRxSKSJyJ5ZWVlAW76GJxuIrEekquhr5RSRwQS+sXAkA7vM4CSALdfDKz3dw15gFeAKZ0XMsY8aYzJNcbkpqamBrjpY3BG4pZWQENfKaU6CiT01wCjRCRLRFzAQmBZgNtfAySKSHuSXwhsOf5iHidHJBHGaunXNunwykop1a7X0Pe30L8NvAVsBZYYYwpE5D4RmQ8gItNEpBi4BnhCRAr863qxunbeE5FNWF1Ffzo1VenAGYnLaEtfKaU6cwSykDFmObC807Sfdni9Bqvbp7t13wGyT6KMx88ZhcNrPRxdQ18ppY4IvTtyAdzx2FpqcNltGvpKKdVBaIZ+ZALSVE1cpFNDXymlOgjR0E+E5mriIx36IBWllOogNEPfnQBtjSS7tU9fKaU6Cs3Qj0wAIC2iRUNfKaU6CM3Qd1uhP9DZpKGvlFIdhGbo+1v6qc5mDX2llOogNEPf39JPtjdS29yGT4dXVkopIFRDPzIRgCRpwBioa9GhGJRSCkI29P0tfYd1V+7B2uZglkYppfqN0Ax9dzxgde8AFFed2ue2KKXUmSI0Q9/uBFcM8dIAwP6qpiAXSCml+ofQDH0AdwJR3jpcdhvFGvpKKQWEcuhHJiLNNaQluNlfraGvlFIQ0qGfAE3VpMREUNnQGuzSKKVUvxC6oe+Oh6YqEqOcVDXqDVpKKQWhHPqRCdBcTWKUiypt6SulFBDSoZ8ETVUkRTmpamzFGL0rVymlQjf0o5LB00yK20eLx0dTmzfYJVJKqaAL7dAHBjnqAbRfXymlCDD0RWSuiGwXkZ0icnc382eLyDoR8YjIgm7mx4nIfhH5fV8UOiD+0E+x+0Nf+/WVUqr30BcRO/AYMA8YB1wvIuM6LbYPWAQ838NmfgF8cOLFPAFRSQAkmjoAqrWlr5RSAbX0pwM7jTG7jTGtwIvAFR0XMMYUGmPyAV/nlUVkKjAQeLsPyhs4f0s/xlcDQL2OtKmUUgGFfjpQ1OF9sX9ar0TEBvwv8J+9LLdYRPJEJK+srCyQTffOH/pRXiv0GzT0lVIqoNCXbqYFev3jHcByY0zRsRYyxjxpjMk1xuSmpqYGuOleuBPA5sDdXA5AQ6uGvlJKOQJYphgY0uF9BlAS4PbPBs4TkTuAGMAlIvXGmC4ng/uczQYJQ4mo2wfM1O4dpZQisNBfA4wSkSxgP7AQ+EogGzfG3ND+WkQWAbmnJfDbJQ3HVr0Hu020e0cppQige8cY4wG+DbwFbAWWGGMKROQ+EZkPICLTRKQYuAZ4QkQKTmWhA5Y0HKksJMZlp6FFb85SSqlAWvoYY5YDyztN+2mH12uwun2OtY2/An897hKejMRMaKkhLaKZumZt6SulVOjekQuHr+AZ6GzS7h2llCLUQz8iDoAUZ7NevaOUUoR66Lv9oe9o1qt3lFKKkA/9eACSHU3UNukwDEopFdqh7+/eSYtoY391k46pr5QKe6Ed+u720G+huc1HWV1LkAuklFLBFdqhf/hErhX2eysbg1kapZQKutAOfZsdXLEk2psA2Fuhoa+UCm+hHfoA7niiTQMAh+qag1wYpZQKrtAP/cgEHM3VRLnsVNTr07OUUuEt9EM/dhDUHyA5xkWlPjJRKRXmwiP06w6QFB1Beb1evaOUCm9hEPppUH+Q1Ci7tvSVUmEvDEJ/EBgfw9wN2qevlAp7YRD6aQAMcdZS0dCCz6d35SqlwlcYhP4gAIa5amnzGg7pXblKqTAWBqFvtfQz7DUA7K1oCGZplFIqqEI/9KMHAEIqlYAOxaCUCm+hH/p2B8QMIM5Tgd0m7NOhGJRSYSyg0BeRuSKyXUR2isjd3cyfLSLrRMQjIgs6TM8RkVUiUiAi+SJyXV8WPmCxg7DVH2BgbASlNToUg1IqfPUa+iJiBx4D5gHjgOtFZFynxfYBi4DnO01vBG4yxowH5gKPiEjCyRb6uMWmQW0pKbERlOkNWkqpMBZIS386sNMYs9sY0wq8CFzRcQFjTKExJh/wdZr+uTFmh/91CXAISO2Tkh+P2EFQV0pKTATlevWOUiqMBRL66UBRh/fF/mnHRUSmAy5g1/Gue9JiB0NjOYOiRIdiUEqFtUBCX7qZdlx3OIlIGvAscIsxxtfN/MUikicieWVlZcez6cC0X6vvrqOioVVv0FJKha1AQr8YGNLhfQZQEugHiEgc8C/gJ8aY1d0tY4x50hiTa4zJTU09Bb0/cYMByLBX4/UZqhp1OAalVHgKJPTXAKNEJEtEXMBCYFkgG/cv/zLwjDHmHydezJPkv0FrkFQB6F25Sqmw1WvoG2M8wLeBt4CtwBJjTIGI3Cci8wFEZJqIFAPXAE+ISIF/9WuB2cAiEdng/8k5JTU5Fn9Lf4BYN2iVVDed9iIopVR/4AhkIWPMcmB5p2k/7fB6DVa3T+f1/gb87STLePIiE8EeQZKnAtDQV0qFr9C/IxdABOLSiGo5hMtuo1hDXykVpsIj9AFiByN1paQluNlfpaGvlApP4RP6cWlQV0p6QqR27yilwlb4hL5/KIb0eDf7NfSVUmEqfEI/bjB4mhge5+FQXQutni73iCmlVMgLn9D335Wb5arFGDigo20qpcJQGIW+da3+UEc1AMXVOq6+Uir8hE/ox1l35ab7Q39TcU0wS6OUUkERPqHvH4ohvq2c0QNj+XDHKRjYTSml+rnwCX1HBMQPgW3/YtbweNburdLRNpVSYSd8Qh/gvP+A0g1MiTxIc5uPA7V6MlcpFV7CK/RTxwCQFWVdp7+7rCGYpVFKqdMuvEI/KhmADJcV9nvK64NZGqWUOu3CLPRTAIjz1RDtsrNLW/pKqTATXqEfmQhiQxoryUqNZne5hr5SKryEV+jbbBCZBI3lDE+J0e4dpVTYCa/QB6tfv6Gc4anRFFc1UdPUFuwSKaXUaRN+oR8zAOoOcPHYgdhE+O27O4JdIqWUOm3CL/RTRkH550wYHMeskSms3l0R7BIppdRpE36hnzoGmquh/hDj0uLYeaheh1lWSoWNgEJfROaKyHYR2Skid3czf7aIrBMRj4gs6DTvZhHZ4f+5ua8KfsJSR1u/D21hbFosrV4fu8r0hK5SKjz0GvoiYgceA+YB44DrRWRcp8X2AYuA5zutmwTcC8wApgP3ikjiyRf7JKTlgM0Bez5g/OA4ALaU1Aa1SEopdboE0tKfDuw0xuw2xrQCLwJXdFzAGFNojMkHOveTfBF4xxhTaYypAt4B5vZBuU9cZAIMOwe2v0FmcjQRDhtbSjX0lVLhIZDQTweKOrwv9k8LREDrishiEckTkbyystMw5PHwOVC2DUdrDWMGxZKnI24qpcJEIKEv3UwLNCEDWtcY86QxJtcYk5uamhrgpk/C4CnW75L1zM9JZ2NRNS+tKz71n6uUUkEWSOgXA0M6vM8ASgLc/smse+oMnmz93r+OW2dlkhLjYpVeuqmUCgOBhP4aYJSIZImIC1gILAtw+28Bl4hIov8E7iX+acEVmQBJI2D/OkSEyUMT2bCvOtilUkqpU67X0DfGeIBvY4X1VmCJMaZARO4TkfkAIjJNRIqBa4AnRKTAv24l8AusHcca4D7/tOBLnwIl6wCYOTyZ3eUNbD9QF+RCKaXUqRXQdfrGmOXGmLOMMSOMMff7p/3UGLPM/3qNMSbDGBNtjEk2xozvsO5TxpiR/p+nT001TsCQGVBXCkWf8eXJ6UQ4bDz36d5gl0oppU6p8Lsjt92k6yE6FVb/gcRoFxePG8jr+aU0t3mDXTKllDplwjf0I2IgYzoc2grAdblDqGxo5a6X8oNcMKWUOnXCN/QBUs+Cil1Q+DGzM6O56exhLN9USnVja7BLppRSp0R4h37KaPC1wV8vgw8e5LppQ2jzGv7y8Z5gl0wppU6J8A79oTOOvK4tYfzgeK7IGcyTH+6mplEfrqKUCj3hHfpJw+Gyh63XLdblml8/bzgtHh9X/3El9S2eIBZOKaX6XniHPsC0r8HoS6HaulxzQno8P5o3hp2H6nnknc+DXDillOpbjmAXoF9IGAa7V4C3DexOvnH+CPZVNvLnj/fQ6vVx+5wRpMVHBruUSil10rSlDzD8fGhrhPfug5INAPzXl8Zx+aTBPLNqL3N+tYJ/5BX1shGllOr/NPQBRl0CsWmw8lH40wUAuJ12fnf9ZN7+/myMgX/k6SicSqkzn4Y+gM0OV//Zem184DtyV+5ZA2O55dxM1u6rIr+4mjavPk9XKXXm0tBvl3kuzP+99bqq8KhZl05Iw+szzP/9J4y65w1+/+8dp798SinVBzT0Oxo0wfpdvOaoyZOGJPD3xTO5ZVYmAL95dwf/s3wrja0e9lU06qWdSqkzhhjTvx4TmJuba/Ly8oLz4T4f/G4yRCXDrW+B3dllkdrmNu5/fSt/zysiJcZFeX0r07OSWPKNs4NQYKWUsojIWmNMbm/LaUu/I5sNzr8b9q+FX6TARw93WSTO7eTBBdn8ffFMyuutMXo+21PJXUvzeXXD/tNdYqWUOi7a0u/O8v+Ez560Xv+kDByubhf7ZGc5T328h4ZWDzsO1lPRYO0EvjF7OHfPG4NId48IVkqpvhdoS19Dvzs+H6x9Gv71H5CeC9nXwZDpMDinx1U8Xh/feWE9b2w+AECk047XGB66OpsrJ6cDUNPYRoTThttpPy3VUEqFDw39k2UMrPkzLP+h9T4xE+7caE3voQXv8xkO1DazalcFD721jYO1LQCcNyqFyycN5n/f3k5zm4/3fnA+KTERp6kiSqlwoKHfVzYthZe+duR98ij4yt8hecQxVyura+Fvq/fyhxU7afN2/Y7/4wtnsaawklaPj3suG0tWSjQuh40Ihx4FKKWOX5+GvojMBX4L2IE/G2Me6DQ/AngGmApUANcZYwpFxAn8GZiCNc7PM8aYXx7rs/pd6AM0VsI/vw473z0y7aJ7Ydad1o1dx2CMIb+4hnX7qnhtYwlNbT62ltZ2u+zc8YP47fU5VDa0UljeiNMu5GYm0erxYbcJdpueI1BKda/PQl9E7MDnwBeAYmANcL0xZkuHZe4Aso0x3xSRhcBVxpjrROQrwHxjzEIRiQK2AHOMMYU9fV6/DH2wunVKN0DhJ/D2Pda0rNmQPhWGX2CN3xOAxlYPTa1env6kkIkZ8YwZFMvFD39w+GggOdpFXbOHVv+dvw9ePZG7XtrENVMz+NU1kwDr/IHDrhdeKaWO6MvQPxv4mTHmi/73PwLo2GIXkbf8y6wSEQdwAEgFFgJfAa4C4oFVwExjTGVPn9dvQ7+dMdBaDwWvwLJvH5k+8mI474cweDI4IqzHMCZl9XokAHCwtplP91Ty3RfWH3O5ey4dS3VTK09+uJunF02nsdVDpMvO4IRIhqdE69VCSoWxQEM/kKGV04GOQ0wWAzN6WsYY4xGRGiAZWApcAZQCUcD3jxX4ZwQRiIiFKV+FjGmwbyXsXQVbX4Od71nL2J3gbbUGcpt8IzSUw5q/WM/kHXGRtW4HA+PczJ80mMzkKOqbPYwaGMv72w4xamAMG4qq+eDzMlZsL+P+5VsPr3PjXz49ahujBsRw23lZ1DZ5+HhnOReNHcD+qiZyM5O4eOwA3SEopYDAWvrXAF80xtzmf/9VYLox5jsdlinwL1Psf78LmA6MAe4AFgGJwEfAPGPM7k6fsRhYDDB06NCpe/fu7ZPKnVYtdbDyd9BcA9vfOPxQlh6ljoUbl0JpPhzYBHPuOubiW0trefLD3VQ0tNLY4mF9UTVfGDuQNwtKgd4D/bcLc8hMjiY7Ix4RwRjT646gtKaJQXFu3WEodQboL907vwdWG2Oe9S/3FPCmMWZJT5/X77t3AmUMVO6GTf+A/L/D9G/AgDHw0m3QUGYtY3dZRwRg7QQmXg0Fr8LYy2H7cogZAJf+GhKHddm8r60FW1sDPHsVB1NmcsPeSzlU28y3LhjJX1cWUlrT3GPRpg5L5PMDdcwamcJVU9LJzogHoKHFy1sFB/B4DZdlD+Lihz/kB184i32VjSRFu/jRpWP7/GtSSvWNvgx9B9aJ3IuA/Vgncr9ijCnosMy3gIkdTuR+2RhzrYjchdXavxWre2cNsNAYk9/T54VM6B9LSx3U7IcPfwXbXgdPzwENwLnft7qFvK3WsM/1B+CDh6DG3+uWmAV3bjh8D0FJZR2Rr32TuLPO5Qf5Gbj2fcA73qlUEUdilJPBtirGNeXxD+/59HSUcPbwZFbtrjhqWuEDl/HvbQcZkhjFqIGxR82rb/EQExFAb+GBTVCcB7m39L6sUipgfX3J5qXAI1iXbD5ljLlfRO4D8owxy0TEDTwLTAYqsYKnlyzwAAASrElEQVR9t4jEAE8D47DS5WljzK+O9VlhEfqdHSwAmxO2vWY9zGXLMph6s/W7eA1U7rLG+T+WmIFQf8hav66ky2xPZCqOeb+Epip44z8BqFr4Gm+XRtG2/kWWN4wmyeXli2kNfGfLGAAGxzqZ2LCSXzr/xMUtv6bWFo/HZ/293HZuFldOTufl9fvx+gwvfLaPm8/J5EfzxlBc1URGYiSFFdZlpxmRHijdCFnnwc+sowruOQDObh5BaQw0V0Nk4ol/n0qFIb05K5TUHYBDW63gb22wAn7TEoiIg5EXwbLvgNisISN2vA1Nnc6V25zg8wA9/1sbsSH+HcvmmHOosKcyu/Z1xBx5oMzjnsv5zDeGB51/4obWH7PDZHC17UMusefxgvdCak0U6VLOv3wzuXrKEFas20wZiSxL+F+ym9dybdRfWNJo3eiWf9lrrCys5dyseCbknmfdC+Fpxnz6BPLJIzw383VWV8Xwm2snYQNsW/4JZ82FiJi+/nZPTGsDOCKtQfr6m4ZycMWA0x3skqjTSEM/nNWWQk2xdRVRVDLEZ1gnllc8aO04hs+xBpH74CFwRsHZ34KCl62upuNwwJ7GIG9pl+krYy6hqaaMi+zrWc4sLuUTALb6hjLWtu+oZVuMg+2pl5Bdvvyo6f/ZtpgtvmGkSC1XDW3iygOPssI2gwRbExEDRtHW3ED2VT+gYcAU7n4pnzsvHM7IQQlHbaOsrgVjDAPi3EeGz/C2Wd9LY6XVVRaTCi31sPklSBpu3YQ3+UbrvErK6K7B6fXAp3+07tWYcTvMe8B66E78kIAuzz1KaT6kjoHX7rTKNP/Rrst4WqyyBxrgxsDPE6x7R2565eh55Ttg9woYlA1DO1yA19YES26yRpjNmHr0Ohuet44gz/3eMeqxEeIyIDo5sDL2V8ZYP6dzR36MYV2Ol4a+Oj7eNusoYejZYHNA1R7rSqSmKti/DhxuGHEhfPiQFVYpZ4G3xZr+pd/Asu/iwyBx6cjmpT1+TEP8SKJrdgLwuWscyb4Kkj0HT7jYb3qnMVr2kSHlOMVLiyMWp8NB2eALWVIUzwgpYW7ifmwHN1FHFLE0Hl7XZ3OxNfkLjPNuQyp3df1K4jPh3Duxp46GYedYAfzuz+DTx48slHMjbPgbTLgakkZYO9CWWmvZmiJrJ5s22brM1+70f5cRsP5v1naGzISi1da27twI25ZblwLHDLC6v/5vPpRthSk3WzcAZkyDz/5k3Qdy1eOw5yMYcQE0VVuDBI6+9PBznlnwlPXvtPFF6w7y3005ch7opldh2CyrTDvegecWQHQqzP5/MOUmayfjbbOGGAfr7vML7rHKbox1NBGTan3ug8MgYRikTYJLfmHtXLYusxocF/+s6z+aMbBvlbW8K/roeU3VEJkADRXW7/YdaWsjrHsGhp0NA8bDgXzrnpj2wPS0WufG3HHH/oNprLS26Y63/o22vW7dY+OOh2e/DCXrrH/D2VYXKNuWWw2iC34Mqx6Dsm1w82tHPretCV653Rqe5cJ7un7eoa3WBRvuBKubdsh0q+FQVwLVRfDcNbDoNesmz5Okoa+Cp2qv9R8qYYh1xJE0ArPr35RVVTMgZx5NJVtYUepk9pQJRLnsbN+9B68zltG7n6a1qpjytghc3iZSUgbQlD6LR59bykgp4bzhsbwiF5O062Uusq8HDM0mgiKTSiMRnG/L5zXf2aRSzUzbVpxidU21Gjsu8fZYXJ8RbGL9P9hkRjA2sorqhPEkln6MXY7z/4fYwXTzWTEDrWAp//z4thf4B3Os7jviMqC2uGuZcm+17jE5uLnD9EGQkdv1yC9+CEy63roqreCfMHa+1fXYvtOCrvVPOcvqdmyssBoNG56zdhh1JVa34/Svw77VMP4qK3ALXoFZ34VPfmutf/5dkDoalt7atU7zfw+FH4OvzTpSAytc3XHWTsgVbV38MOJCGDDW2qG0j6M1YYH1t1m02qrXRT+1jvKiU63yzbnbOoLZvrzr5876nnU3fsxA64hv9wqwR8ANS+CTR62d89j5sHclvHqHtY4rFlrrrJ1ne93aDZtlLV+xA2be0eu4Xj3R0Fcho6qhlTafjwGxbowxfH6w3roZ7Zk8JmUkcFl2Gj95ZRPjkm2UtboQYH9ZORcMjyV/0wa2mUxiTS3R8cnsLD6EHR8VxOFDuMn+Np/6xnLIJHBz/HpeappKUat13uAsKSJDypgSU4m3sZoIp5OC1gFs8Q2lDQdnpzuYO6iet9bvYqgcotikcut4O5+VttEgMWwu9zJpkJspUQexV+1iZFIEnugBrN91gF32LM4/K4Uh6ek8XuBgUuUbzEhz4srIwZacBUVr8JZuwsx7CIcYyPsL2Bx496/nd/UXck16BcmF/+KF2my+4l5J/YCpOO024kpXYjwtmCEzwNOCLT4dij6zgjFjunUUZ7Ph2fMxjuYq6wu2Oa0WdP0hqyXbUfJIqNhpXVIcNxh2/ZsuO5fkkdaRx+HpveyA2kUmdT3/dCxTboI9H3Z5hvVJSR1r7ZAaDlnnaL7yIjxzhTUvIh5aanrfRndBbnNa33m7tElWuZt72V7KaLhj9Ql1MWnoK9WNraW1VDa04nLY8PkMM4Yn86/8UrzGcHl2Gp/tqeSRd3dw0dgBbCyuYW1hJQPi3DS2eqhubGNMWhw/+MJZvLJhP/+3shDfKfjvM3pgLFkp0bxZcIDhqdH8z1UTefGzfUzMSCAt3s0dz60DIGdIAhuKqskZHM2GkgYAJqTHsXm/NaDf0KQoRg2I4WvnZXHOCKubxhjD21sO8o1n1wLw+k1DaSECe+wAcoYk4KmvwLz9X1Tk3M7W2kiS4mOYlOSB2MFWENUU46k9SKFjGCObNsOwc8HusMKsfIfVbZNzg3X11fq/QVq2dUmx2GDHW1C9D6beYnV3eZr93Vm58PmbVndH5rnWkcT4q6ydTWMFLL0FZnwT5j1ofUEtdVbX0ye/tVr1ySOt7pHoVKgrhagU60KHoWdbn9FSbw2dsucj69zWBT+2lt23CsZ8yTpa2fE2ZJ4HKSPh3/fDxhesR6aufdo6v7PnI6trK+cG64ikttS6OCIx09phbllmjc019nJY/bhV3yEzYOB4q0sHrB2jMdaObvty6ygo81zrjv4h06wdckM5pE85ob8bDX2l+ljnu5jzCit5Pb+UW2ZlUtXYxoDYCJbkFXFt7hAcNqG5zUdpTRMjBsTw/b9v4KMd5Vw4ZgBfPXsYRZWN/PWTQhx2YWCcm492lB/e7phBsWw/WEdv/zXjI53UNB1pTU5Mj2fT/u5bkjOykoiPdDI8NYY/ftD1/AXA+MFxFJRYOwwRDn/+tbkZfLyjnEsnpuF02NhX2ci/8kv5++KZbCyuJsrl4JrcDCIcduqa22ho8dLQ6mFgnJt3txxka2ktY9PieD2/hC9lD+ZL2Wm8s+Ugr24o4RdXTsBnDAPjuj9RbYyB8s/5/puVjBk6gG+ef6Tro7y+hd1lDSzJK+Inl43F6zMkd3hORX5xNS98to//vnLi8Y9Q24cnWI/erGH5pgNcMCaVKFcgo+AETkNfqX6kxePFLtLj6Kgrth8ir7CKH35xNACtHh91zW088u4Onl29lzvmjOCf6/bT0OLh0a9MJjUmgjGDYikoqeW7L67ntnOz+OrZmTy7qpCnVxbyxp3nce0Tq9lYVE18pJOEKCd7K6yT2C677fAorueNSjlqh3Mypg5LZO3eqhNa974rxrPjYD11zW1MHZbIyl0VJEW7eO7To6/2evDqiUxMT+Bnywr4rPDorqGYCAdr/+tijAGHTTjvofcprWnm+dtmkDM0gTaPodnjJSUmgvteK+DsEcmcOyqV1zeW8PQnhczPGczM4UnkDEmktqmNxOgjj0l97tO9xEQ4uCLHegreodpmfAbiIh3YbcLzn+7j6qkZxLmdx6zn2wUHWPzsWq6anM5vrjv6SXwnO3quhr5SIaDV42PZxhIunTiIplYvbqed6EDufAbqmtuobmxjSFIUAB9+XkZpTRNXT8ngs8JKpmcm4bDbaPP6aGrz4vUafMaQFO3CGKhpauNgXTMNLR4cNhu/ems7EQ4bW0trGZocxerdlfzPVRPx+Hzcu6zg8JHBlyenU1bfcnhncsm4gRysa2FiehzREQ6e+MAaemtCehwNLV72lDec8PfT8egEIM7toK7Fc9RR0rDkKMrqWmhstU4wD4pzc6DWugs+KyW6x8/PSIzk7nljeLvgIMs2Wjc8zp80mPL6FlbuquiyfEpMBH+5OZfKxlbWFlbxya5yiquauDJnMHl7q8hMjubl9fsBa6e08kcXkhjl4p6XN3H99KE8s2ovtU1t/GXRtBP6LjT0lVJ9rj0vfAY27a9hkn8AP4/XR0VDK1WNrYwZFHd42foWD7GdWr+tHh/Pf7qXy7IHEx/p5NM9Ffxy+TYWzcokIzGSbz67ltpmD3NGp/LVmcM4/6xUDPDaxhIKyxvwGkNSdARZKVFcOGYgh/wBvmp3Bf/IKyYzJYrPD9ZTVtfC5ZMG89j7O/F2OPlit8lR71NiIiivbyElJoKkaCefH6zvUu+0eDexbgetHh/7q5u6fRpepNNOU9uRK5eiXPbDO5rOnHbB7bQT53ayv7rp8PSF04bwwNXZvf0zdEtDXyl1RvL5DLY+eEpc+3bqmtuIdNr597ZDjBkUR1KMC5twVJ/6puIaMhIjcTps1DS14bLb+PNHu1m3r4r7r5rIWR3GmqppbCPSZeeTXeUMSYzkv14pICs1mjvmjOCZVXsZHO/mnJEpZCZbRxGPr9jJKxuODI1y66wsbpg5lF+8voXVuytobjsyxMozt05n9lmpJ1RfDX2llOondpXV8/K6/XzrgpFEuo7cud1+5LB6dwVZKdHMyEo64aHMNfSVUiqMBBr6/XC0KKWUUqeKhr5SSoURDX2llAojGvpKKRVGNPSVUiqMaOgrpVQY0dBXSqkwoqGvlFJhpN/dnCUiZcDek9hECtA3wwb2b1rP0BMuddV6nhrDjDG9juHQ70L/ZIlIXiB3pZ3ptJ6hJ1zqqvUMLu3eUUqpMKKhr5RSYSQUQ//JYBfgNNF6hp5wqavWM4hCrk9fKaVUz0Kxpa+UUqoHIRP6IjJXRLaLyE4RuTvY5TlZIvKUiBwSkc0dpiWJyDsissP/O9E/XUTkUX/d80VkSvBKfnxEZIiIvC8iW0WkQETu9E8PqbqKiFtEPhORjf56/tw/PUtEPvXX8+8i4vJPj/C/3+mfnxnM8h8vEbGLyHoRed3/PlTrWSgim0Rkg4jk+af167/dkAh9EbEDjwHzgHHA9SIyLrilOml/BeZ2mnY38J4xZhTwnv89WPUe5f9ZDDx+msrYFzzAD4wxY4GZwLf8/3ahVtcW4EJjzCQgB5grIjOBB4Hf+OtZBXzNv/zXgCpjzEjgN/7lziR3Als7vA/VegJcYIzJ6XB5Zv/+2zXGnPE/wNnAWx3e/wj4UbDL1Qf1ygQ2d3i/HUjzv04DtvtfPwFc391yZ9oP8CrwhVCuKxAFrANmYN284/BPP/x3DLwFnO1/7fAvJ8Eue4D1y8AKuwuB1wEJxXr6y1wIpHSa1q//dkOipQ+kA0Ud3hf7p4WagcaYUgD/7wH+6SFRf/+h/WTgU0Kwrv4ujw3AIeAdYBdQbYzx+BfpWJfD9fTPrwGST2+JT9gjwP8D2p/4nUxo1hPAAG+LyFoRWeyf1q//dh29L3JG6O5JwuF0WdIZX38RiQFeAr5njKk9xsOhz9i6GmO8QI6IJAAvA2O7W8z/+4ysp4h8CThkjFkrInPaJ3ez6Bldzw5mGWNKRGQA8I6IbDvGsv2irqHS0i8GhnR4nwGUBKksp9JBEUkD8P8+5J9+RtdfRJxYgf+cMeaf/skhWVcAY0w1sALrHEaCiLQ3vjrW5XA9/fPjgcrTW9ITMguYLyKFwItYXTyPEHr1BMAYU+L/fQhrRz6dfv63GyqhvwYY5b9CwAUsBJYFuUynwjLgZv/rm7H6v9un3+S/OmAmUNN+eNnfidWk/wuw1RjzcIdZIVVXEUn1t/ARkUjgYqwTne8DC/yLda5ne/0XAP82/o7g/swY8yNjTIYxJhPr/+G/jTE3EGL1BBCRaBGJbX8NXAJspr//7Qb7REgfnlC5FPgcq5/0nmCXpw/q8wJQCrRhtRC+htXX+R6ww/87yb+sYF29tAvYBOQGu/zHUc9zsQ5x84EN/p9LQ62uQDaw3l/PzcBP/dOHA58BO4F/ABH+6W7/+53++cODXYcTqPMc4PVQrae/Thv9PwXtudPf/3b1jlyllAojodK9o5RSKgAa+kopFUY09JVSKoxo6CulVBjR0FdKqTCioa+UUmFEQ18ppcKIhr5SSoWR/w+wvmxReUKNVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(AOf)\n",
    "\n",
    "x_test, y_test, x_val, y_val, x_train, y_train = AOf.split_into_parts(a_data, test_part=0,val_part=1, embed_file=embedding_file)\n",
    "\n",
    "hps = {'N_BATCH': 96,\n",
    "       'y_out': y_test.shape[1]}\n",
    "name = 'AnOxPePred'\n",
    "\n",
    "model = 'model'\n",
    "tf.keras.backend.clear_session()\n",
    "del model\n",
    "\n",
    "model = AOf.create_AnOxPePred_v1(hps)\n",
    "\n",
    "EPOCHS = 800 # 800 seems optimal\n",
    "\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=100, restore_best_weights=True, verbose=0)\n",
    "history = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=hps['N_BATCH'], \n",
    "                    callbacks=[es], validation_data=(x_val, y_val), class_weight=None, verbose=1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "AOf.calc_metrics(y_test, y_pred, idx='metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested cross-validation (Performance measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(AOf)\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "pred_df = pd.DataFrame()\n",
    "PAR_VEC = [4,3,2,1,0]\n",
    "EPOCHS = 800# 50 seems optimal\n",
    "\n",
    "# Loop training\n",
    "for te_p in set(PAR_VEC):\n",
    "    for va_p in set(PAR_VEC):\n",
    "        if te_p != va_p:\n",
    "            \n",
    "            x_test, y_test, x_val, y_val, x_train, y_train = AOf.split_into_parts(a_data, test_part=te_p,val_part=va_p, \n",
    "                                                                                  embed_file=embedding_file)\n",
    "\n",
    "            hps = {'N_BATCH': 96,\n",
    "                   'y_out': y_test.shape[1]}\n",
    "            \n",
    "            model = 'model'\n",
    "            tf.keras.backend.clear_session()\n",
    "            del model\n",
    "\n",
    "            model = AOf.create_AnOxPePred_v1(hps)\n",
    "\n",
    "            es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=100, restore_best_weights=True, verbose=0)\n",
    "\n",
    "            model.fit(x_train, y_train, epochs=EPOCHS, batch_size=hps['N_BATCH'], \n",
    "                                callbacks=[es], validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "            y_pred = model.predict(x_test)\n",
    "            name = str(te_p)+'_'+str(va_p)\n",
    "            \n",
    "            result_df = pd.concat([result_df, AOf.calc_metrics(y_test, y_pred, idx=name)])\n",
    "            pred_df = pd.concat([pred_df, AOf.output_df(x_test, y_test, y_pred, tag=name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.5_AUC</th>\n",
       "      <th>0.5_F1</th>\n",
       "      <th>0.5_MCC</th>\n",
       "      <th>0.5_AUC</th>\n",
       "      <th>0.5_F1</th>\n",
       "      <th>0.5_MCC</th>\n",
       "      <th>custom_AUC</th>\n",
       "      <th>custom_F1</th>\n",
       "      <th>custom_MCC</th>\n",
       "      <th>custom_AUC</th>\n",
       "      <th>custom_F1</th>\n",
       "      <th>custom_MCC</th>\n",
       "      <th>FRS_Threshold</th>\n",
       "      <th>CHEL_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2'mers</th>\n",
       "      <td>0.845767</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.297664</td>\n",
       "      <td>0.845636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845767</td>\n",
       "      <td>0.801887</td>\n",
       "      <td>0.654447</td>\n",
       "      <td>0.845636</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.651169</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3'mers</th>\n",
       "      <td>0.895849</td>\n",
       "      <td>0.638102</td>\n",
       "      <td>0.528087</td>\n",
       "      <td>0.710318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.895849</td>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.676632</td>\n",
       "      <td>0.710318</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.168475</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4'mers</th>\n",
       "      <td>0.728452</td>\n",
       "      <td>0.417234</td>\n",
       "      <td>0.311555</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728452</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.368394</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.403781</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5'mers</th>\n",
       "      <td>0.726910</td>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.160073</td>\n",
       "      <td>0.644393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.726910</td>\n",
       "      <td>0.730556</td>\n",
       "      <td>0.374382</td>\n",
       "      <td>0.644393</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.176150</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-10'mers</th>\n",
       "      <td>0.738643</td>\n",
       "      <td>0.325359</td>\n",
       "      <td>0.214691</td>\n",
       "      <td>0.480403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.738643</td>\n",
       "      <td>0.674403</td>\n",
       "      <td>0.379809</td>\n",
       "      <td>0.480403</td>\n",
       "      <td>0.080940</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11-15'mers</th>\n",
       "      <td>0.706503</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>0.270776</td>\n",
       "      <td>0.569940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706503</td>\n",
       "      <td>0.629969</td>\n",
       "      <td>0.322270</td>\n",
       "      <td>0.569940</td>\n",
       "      <td>0.121622</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16-30'mers</th>\n",
       "      <td>0.554147</td>\n",
       "      <td>0.198347</td>\n",
       "      <td>-0.038180</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554147</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.142084</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.488445</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0.5_AUC    0.5_F1   0.5_MCC   0.5_AUC  0.5_F1  0.5_MCC  \\\n",
       "2'mers      0.845767  0.362319  0.297664  0.845636     0.0      0.0   \n",
       "3'mers      0.895849  0.638102  0.528087  0.710318     0.0      0.0   \n",
       "4'mers      0.728452  0.417234  0.311555  0.656716     0.0      0.0   \n",
       "5'mers      0.726910  0.321101  0.160073  0.644393     0.0      0.0   \n",
       "6-10'mers   0.738643  0.325359  0.214691  0.480403     0.0      0.0   \n",
       "11-15'mers  0.706503  0.309859  0.270776  0.569940     0.0      0.0   \n",
       "16-30'mers  0.554147  0.198347 -0.038180  0.648936     0.0      0.0   \n",
       "\n",
       "            custom_AUC  custom_F1  custom_MCC  custom_AUC  custom_F1  \\\n",
       "2'mers        0.845767   0.801887    0.654447    0.845636   0.640000   \n",
       "3'mers        0.895849   0.837255    0.676632    0.710318   0.153846   \n",
       "4'mers        0.728452   0.635417    0.368394    0.656716   0.464286   \n",
       "5'mers        0.726910   0.730556    0.374382    0.644393   0.264706   \n",
       "6-10'mers     0.738643   0.674403    0.379809    0.480403   0.080940   \n",
       "11-15'mers    0.706503   0.629969    0.322270    0.569940   0.121622   \n",
       "16-30'mers    0.554147   0.556701    0.142084    0.648936   0.400000   \n",
       "\n",
       "            custom_MCC  FRS_Threshold  CHEL_Threshold  \n",
       "2'mers        0.651169           0.44            0.32  \n",
       "3'mers        0.168475           0.43            0.29  \n",
       "4'mers        0.403781           0.44            0.29  \n",
       "5'mers        0.176150           0.39            0.27  \n",
       "6-10'mers     0.005881           0.39            0.23  \n",
       "11-15'mers    0.072900           0.42            0.18  \n",
       "16-30'mers    0.488445           0.40            0.32  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(AOf)\n",
    "tmp_df = pred_df.copy()\n",
    "custom_bins = [[2],[3],[4],[5],range(6,11),range(11,16),range(16,31)]\n",
    "new_x = [\"2'mers\",\"3'mers\", \"4'mers\", \"5'mers\", \"6-10'mers\", \"11-15'mers\",\"16-30'mers\"]\n",
    "\n",
    "single_length_results = AOf.length_metrics(tmp_df, custom_bins, new_x)\n",
    "single_length_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.5_AUC</th>\n",
       "      <th>0.5_F1</th>\n",
       "      <th>0.5_MCC</th>\n",
       "      <th>0.5_AUC</th>\n",
       "      <th>0.5_F1</th>\n",
       "      <th>0.5_MCC</th>\n",
       "      <th>custom_AUC</th>\n",
       "      <th>custom_F1</th>\n",
       "      <th>custom_MCC</th>\n",
       "      <th>custom_AUC</th>\n",
       "      <th>custom_F1</th>\n",
       "      <th>custom_MCC</th>\n",
       "      <th>FRS_Threshold</th>\n",
       "      <th>CHEL_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.787537</td>\n",
       "      <td>0.456873</td>\n",
       "      <td>0.343982</td>\n",
       "      <td>0.593357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.787537</td>\n",
       "      <td>0.712603</td>\n",
       "      <td>0.480660</td>\n",
       "      <td>0.593357</td>\n",
       "      <td>0.266220</td>\n",
       "      <td>0.281087</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017091</td>\n",
       "      <td>0.050565</td>\n",
       "      <td>0.033368</td>\n",
       "      <td>0.088857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017091</td>\n",
       "      <td>0.036501</td>\n",
       "      <td>0.044046</td>\n",
       "      <td>0.088857</td>\n",
       "      <td>0.138793</td>\n",
       "      <td>0.141055</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>0.028544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0.5_AUC    0.5_F1   0.5_MCC   0.5_AUC  0.5_F1  0.5_MCC  custom_AUC  \\\n",
       "0  0.787537  0.456873  0.343982  0.593357     0.0      0.0    0.787537   \n",
       "1  0.017091  0.050565  0.033368  0.088857     0.0      0.0    0.017091   \n",
       "\n",
       "   custom_F1  custom_MCC  custom_AUC  custom_F1  custom_MCC  FRS_Threshold  \\\n",
       "0   0.712603    0.480660    0.593357   0.266220    0.281087       0.427500   \n",
       "1   0.036501    0.044046    0.088857   0.138793    0.141055       0.018317   \n",
       "\n",
       "   CHEL_Threshold  \n",
       "0        0.304000  \n",
       "1        0.028544  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([result_df.mean(),result_df.std()],axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(os.path.join(result_path,'04_AO_p70_results.csv'))\n",
    "single_length_results.to_csv(os.path.join(result_path,'04_AO_p70_single_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e43b334f60>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(AOf)\n",
    "\n",
    "_, _, _, _, x_train, y_train = AOf.split_into_parts(a_data, test_part='',val_part='', embed_file=embedding_file)\n",
    "\n",
    "\n",
    "hps = {'N_BATCH': 96,\n",
    "       'y_out': y_train.shape[1]}\n",
    "name = 'AnOxPePred'\n",
    "\n",
    "model = 'model'\n",
    "tf.keras.backend.clear_session()\n",
    "del model\n",
    "\n",
    "model = AOf.create_AnOxPePred_v1(hps)\n",
    "\n",
    "EPOCHS = 400 # \n",
    "\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=hps['N_BATCH'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x000001E439ADD630>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(os.path.join(result_path,'AnOxPePred_v1'), save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.370809</td>\n",
       "      <td>0.231568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.309631</td>\n",
       "      <td>0.213167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.447740</td>\n",
       "      <td>0.161489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.457819</td>\n",
       "      <td>0.205531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.310857</td>\n",
       "      <td>0.228096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.305323</td>\n",
       "      <td>0.154030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.298936</td>\n",
       "      <td>0.232231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.465277</td>\n",
       "      <td>0.179562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.454689</td>\n",
       "      <td>0.163872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.556408</td>\n",
       "      <td>0.343867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.358294</td>\n",
       "      <td>0.278536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1\n",
       "0   0.370809  0.231568\n",
       "1   0.309631  0.213167\n",
       "2   0.447740  0.161489\n",
       "3   0.457819  0.205531\n",
       "4   0.310857  0.228096\n",
       "5   0.305323  0.154030\n",
       "6   0.298936  0.232231\n",
       "7   0.465277  0.179562\n",
       "8   0.454689  0.163872\n",
       "9   0.556408  0.343867\n",
       "10  0.358294  0.278536"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptides = AOf.data_augmentation(np.array(\n",
    "    ['LNIQFNIPTPKLC', 'CPFSSDDQFCLKVGV', 'FCLKVGVVHQNGKRRLALVKDNP','FCLKVGVIHQNGKRRLALVK', 'SSDDQFCLKVGVV',\n",
    "     'KKPVSKDSPETYEEALKRFAKLLSDRKKL', 'DSPETYEEALKRFAKLLSD', 'FDVIGGTSTGGLLTAMITTPNENNRP', 'GIKGIIPAIILEFLEGQLQEVDNNKDAR',\n",
    "     'HHLP', 'LLLLLL']), embedding_file)\n",
    "pd.DataFrame(model.predict(peptides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
